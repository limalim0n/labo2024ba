{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b88daf-e9f6-43be-8cbf-d19fd22519ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_optimizer as optim_mod\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "from IPython.display import Javascript, display\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "general_logger = logging.getLogger('general_logger')\n",
    "general_handler = logging.FileHandler('NoForceTransformers_log.txt', mode='w')\n",
    "general_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "general_logger.setLevel(logging.INFO)\n",
    "general_logger.addHandler(general_handler)\n",
    "\n",
    "\n",
    "def log(message):\n",
    "    general_logger.info(message) \n",
    "    print(message)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ff8466-1c08-4067-b592-e20c867eed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params={'lr': 0.0007379903593864751, 'dropout': 0.11402389385748135, 'batch_size': 619, 'num_layers': 4, 'd_model': 493,\n",
    "         'weight_baja1': 13.887450747162747, 'weight_baja2': 70.9946897814277, \n",
    "         'weight_continua': 1.8534423980414036, 'weight_decay': 0.0003093598055270367}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd37291-3fc3-4af1-bd38-288b27171918",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params={'lr': 0.0007379903593864751, 'dropout': 0.11402389385748135, 'batch_size': 608, 'num_layers': 4, 'd_model': 512, \n",
    "        'weight_baja1': 10.887450747162747, 'weight_baja2': 80.9946897814277, \n",
    "        'weight_continua': 1.534423980414036, 'weight_decay': 0.0003093598055270367, 'dim_feedforward': 949, \n",
    "        'beta1': 0.9279914312981336, 'beta2': 0.9729720290390763, 'eps': 3.9267980177920674e-06, 'grad_clip': 0.8245199110894906, \n",
    "        'gamma': 3, 'scheduler_patience': 7, 'scheduler_factor': 0.10134679071433805}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91d4af9-f3d3-4c01-bc3a-b336603f34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params={'lr': 1e-07, 'dropout': 0.1, 'batch_size': 400, 'num_layers': 8, 'd_model': 420, \n",
    "         'weight_baja1': 60, 'weight_baja2': 70, 'weight_continua': 1, \n",
    "         'weight_decay': 5e-5, 'dim_feedforward': 1000, 'beta1': 0.93, 'beta2': 0.97, \n",
    "         'eps': 2.1e-06, 'grad_clip': 0.69, 'gamma': 4.0, 'reward_baja_2': 117, \n",
    "         'penalty_baja_2': -3, 'miss_baja_2_penalty': -100, 'scheduler_patience': 2, 'scheduler_factor': 0.69, \n",
    "         'num_heads': 10, 'factor': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1083ea5-e9f2-4da8-9c97-b1e9752a8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = Params['batch_size']\n",
    "val_batch_size = 200\n",
    "\n",
    "#Model Parameters\n",
    "input_dim = 306\n",
    "num_classes = 5\n",
    "num_heads = 4 \n",
    "d_model = Params['d_model']    \n",
    "dim_feedforward=Params['dim_feedforward'] \n",
    "dropout = Params['dropout']\n",
    "num_layers = Params['num_layers']   \n",
    "  \n",
    "\n",
    "beta1 = Params['beta1']\n",
    "beta2 = Params['beta2']\n",
    "eps = Params['eps']\n",
    "\n",
    "#Train Parameters\n",
    "num_epochs = 300\n",
    "learning_rate= Params['lr']\n",
    "weight_decay = Params['weight_decay']\n",
    "\n",
    "#Loss parameters\n",
    "gamma = Params['gamma']\n",
    "miss_baja_2_penalty = Params['miss_baja_2_penalty']\n",
    "reward_baja_2 = Params['reward_baja_2']\n",
    "penalty_baja_2 = Params['penalty_baja_2']\n",
    "\n",
    "\n",
    "#Split Parameters\n",
    "split_by_clients = True\n",
    "client_split_ratios=(0.8, 0.2, 0.0)\n",
    "\n",
    "train_steps = 29\n",
    "val_steps = 2\n",
    "test_steps = 0\n",
    "\n",
    "REDUCE = False\n",
    "num_clients = 20000\n",
    "\n",
    "UNDERSAMPLE = False\n",
    "majority_class = 5  \n",
    "padding_label = 69   \n",
    "undersample_percentage = 0.5\n",
    "\n",
    "\n",
    "#Choose Model\n",
    "useTRANSFORMERS = False\n",
    "useTRANSFORMERSHYBRID = False\n",
    "\n",
    "useLSTM = False\n",
    "useLSTMAttention = False\n",
    "useLSTMWeightAtention = False\n",
    "useLSTMEncoder = False\n",
    "\n",
    "#For Actor Critic network\n",
    "useACTORCRITIC = False\n",
    "withSOFT = False\n",
    "withSOFTDISCOUNT = False\n",
    "\n",
    "useSACMH = True\n",
    "\n",
    "setCLASSWEIGHTS = True\n",
    "class_weights_vector = [Params['weight_baja1'] , Params['weight_baja2'] , Params['weight_continua']]\n",
    "\n",
    "useCUSTOMLOSS = True\n",
    "useYOGI = True\n",
    "\n",
    "#Run optuna\n",
    "runOPTUNA = False\n",
    "n_trials = 100\n",
    "\n",
    "#Print statements\n",
    "print_plot = True\n",
    "print_class_report = True\n",
    "print_class_count = True\n",
    "\n",
    "print_grid_report = True\n",
    "print_grid_count = False\n",
    "print_grid_chunks = True\n",
    "\n",
    "\n",
    "if useACTORCRITIC or useTRANSFORMERS or useTRANSFORMERSHYBRID or useSACMH:\n",
    "    num_heads = Params['num_heads']\n",
    "    factor = Params['factor']\n",
    "    d_model = num_heads * factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8d1f33-401e-4eda-9a1a-d05222d9b67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Params['factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7abed4b-fb1b-41e7-9707-56f28c5f9c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Params['d_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5e9d98-97af-4b9c-8dd9-432a5bc63731",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    'useTRANSFORMERS': useTRANSFORMERS,\n",
    "    'useLSTM': useLSTM,\n",
    "    'useLSTMAttention': useLSTMAttention,\n",
    "    'useLSTMWeightAtention': useLSTMWeightAtention,\n",
    "    'useLSTMEncoder': useLSTMEncoder\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21eb1d70-c60f-40cd-9044-69cab893147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_model = [var_name for var_name, var_value in variables.items() if var_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eced1248-70da-4ee5-828d-96b3c1636eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "gc.collect()\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35bee49-550a-4aaf-a73a-793e447401f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(183715, 33, 307)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sequences and labels arrays\n",
    "loaded_data_sequences = np.load('sequence_array_filtered_TRAINING3_final.npz', allow_pickle=True)\n",
    "loaded_data_labels = np.load('label_array_filtered_TRAINING_regresion_final.npz', allow_pickle=True)\n",
    "\n",
    "# Access the saved arrays\n",
    "sequence_array = loaded_data_sequences['sequences']\n",
    "label_array = loaded_data_labels['labels']\n",
    "\n",
    "print(\"Arrays loaded successfully!\")\n",
    "\n",
    "sequence_array.shape[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb34993b-6e72-4dfa-9728-b0453e30156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183715, 33, 307)\n",
      "(183715, 31)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_array.shape[:])\n",
    "print(label_array.shape[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e7a20c-56de-4cae-a8fe-ae78ffefd474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after cleaning:\n",
      "sequence_array_cleaned shape: (183715, 31, 306)\n",
      "label_array_cleaned shape: (183715, 31)\n"
     ]
    }
   ],
   "source": [
    "# Remove the first feature (client ID) from the sequence array and the last two timesteps\n",
    "sequence_array = sequence_array[:, :-2, 1:]  # Remove client ID (first feature) and last two timesteps\n",
    "#label_array = label_array[:, :-2]  # Remove the last two timesteps from the label array\n",
    "\n",
    "print(\"Shapes after cleaning:\")\n",
    "print(\"sequence_array_cleaned shape:\", sequence_array.shape)\n",
    "print(\"label_array_cleaned shape:\", label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e54f88bc-2c6b-430f-b095-894041760b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  4,  3,  2,  1, 69, 69])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array[12943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e4a322-2024-4880-ae07-c23035b6bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [to_cpu_numpy(item) for item in x]\n",
    "    elif isinstance(x, dict):\n",
    "        return {key: to_cpu_numpy(value) for key, value in x.items()}\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9cd8dfe-b102-486e-a3ce-e153c20ad1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(epochs_list, train_loss_values, loss_values, ganancia_values, argmax_ganancia_values, ganancia_threshold_values,\n",
    "                          best_threshold_values, entropy_loss_values, threshold1_values_best , threshold2_values_best, thresholdc_values_best, threshold1_values, threshold2_values, thresholdc_values):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    epochs_list = to_cpu_numpy(epochs_list)\n",
    "    train_loss_values = to_cpu_numpy(train_loss_values)\n",
    "    loss_values = to_cpu_numpy(loss_values)\n",
    "    ganancia_values = to_cpu_numpy(ganancia_values)\n",
    "    argmax_ganancia_values = to_cpu_numpy(argmax_ganancia_values)\n",
    "    ganancia_threshold_values = to_cpu_numpy(ganancia_threshold_values)\n",
    "    best_threshold_values = to_cpu_numpy(best_threshold_values)\n",
    "    entropy_loss_values = to_cpu_numpy(entropy_loss_values)\n",
    "    threshold1_values_best = to_cpu_numpy(threshold1_values_best)\n",
    "    threshold2_values_best = to_cpu_numpy(threshold2_values_best)\n",
    "    thresholdc_values_best = to_cpu_numpy(thresholdc_values_best)\n",
    "    threshold1_values = to_cpu_numpy(threshold1_values)\n",
    "    threshold2_values = to_cpu_numpy(threshold2_values)\n",
    "    thresholdc_values = to_cpu_numpy(thresholdc_values)\n",
    "\n",
    "    threshold1_values = [thresholds[0] for thresholds in best_threshold_values]\n",
    "    threshold2_values = [thresholds[1] for thresholds in best_threshold_values]\n",
    "    threshold3_values = [thresholds[2] for thresholds in best_threshold_values]\n",
    "    threshold4_values = [thresholds[3] for thresholds in best_threshold_values]\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(7, 10))\n",
    "    \n",
    "    # Plotting Train Loss\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(epochs_list, train_loss_values, label='Train Loss', marker='o', color='red')\n",
    "    plt.title(f'Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(min(min(train_loss_values),0), max(train_loss_values) + 0.1)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Best Test Loss\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(epochs_list, loss_values, label='Test Loss', marker='o')\n",
    "    plt.title(f'Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, max(loss_values))\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Best Ganancia\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(epochs_list, ganancia_values, label='Best Ganancia', marker='o', color='green')\n",
    "    plt.plot(epochs_list, ganancia_threshold_values, label='Best T. Ganancia', marker='x', color='red')\n",
    "    plt.title(f'Ganancia')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Ganancia')\n",
    "    plt.ylim(min(min(ganancia_values),0), max(max(ganancia_values),max(ganancia_threshold_values))+1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Best Ganancia\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(epochs_list, argmax_ganancia_values, label='Best Ganancia All', marker='x', color='red')\n",
    "    plt.title(f'Ganancia All')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Ganancia')\n",
    "    plt.ylim(min(min(argmax_ganancia_values),0), max(argmax_ganancia_values)+1.0)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Threshold\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(epochs_list, threshold1_values, label='T1', marker='x', color='red')\n",
    "    plt.plot(epochs_list, threshold2_values, label='T2', marker='x', color='green')\n",
    "    plt.plot(epochs_list, threshold3_values, label='T3', marker='x', color='blue')\n",
    "    plt.plot(epochs_list, threshold4_values, label='T4', marker='x', color='purple')\n",
    "    plt.title(f'Thresholds')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Thresholds')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"ActorCritic_training_metrics.png\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb74efa6-e71f-4605-9f0e-1a831dc9229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards_ordinal_vectorized(preds, labels, mask=None, device='cpu'):\n",
    "    num_combinations, batch_size, seq_len = preds.shape\n",
    "\n",
    "    # Expand labels and mask to match preds\n",
    "    labels_expanded = labels.unsqueeze(0).expand(num_combinations, -1, -1)  # [num_combinations, batch_size, seq_len]\n",
    "    if mask is not None:\n",
    "        mask_expanded = mask.unsqueeze(0).expand(num_combinations, -1, -1)\n",
    "    else:\n",
    "        mask_expanded = None\n",
    "\n",
    "    # Ensure preds and labels are of integer type\n",
    "    preds = preds.long()\n",
    "    labels_expanded = labels_expanded.long()\n",
    "\n",
    "    # Check for invalid label or prediction values\n",
    "    valid_classes = [0, 1, 2, 3, 4, 5]\n",
    "    if not torch.isin(labels_expanded, torch.tensor(valid_classes, device=device)).all():\n",
    "        print(f\"labels_expanded: {labels_expanded}\")\n",
    "        raise ValueError(\"Found invalid labels.\")\n",
    "    if not torch.isin(preds, torch.tensor(valid_classes, device=device)).all():\n",
    "        print(f\"preds: {preds}\")\n",
    "        raise ValueError(\"Found invalid predictions.\")\n",
    "\n",
    "    # Define label rewards\n",
    "    label_rewards = torch.tensor([0.0, 117.0, 0.0, 0.0, 0.0, 0.0], device=device)  # Adjusted rewards\n",
    "\n",
    "    # Initialize rewards tensor\n",
    "    rewards = torch.zeros_like(preds, dtype=torch.float, device=device)  # [num_combinations, batch_size, seq_len]\n",
    "\n",
    "    # ===== Correct Predictions =====\n",
    "    correct_predictions = (preds == labels_expanded)  # [num_combinations, batch_size, seq_len]\n",
    "    rewards += correct_predictions.float() * label_rewards[labels_expanded]  # Assign rewards for correct predictions\n",
    "\n",
    "    # ===== Incorrect Predictions =====\n",
    "    # Define penalty matrix\n",
    "    penalty_matrix = torch.zeros(6, 6, device=device)  # [6,6]\n",
    "\n",
    "    # Populate the penalty matrix based on misclassification rules\n",
    "    penalty_matrix[1, 0] = -3.0     # Predicted 'BAJA+2' when true label is 'BAJA+1'\n",
    "    penalty_matrix[1, 2] = -3.0     # Predicted 'BAJA+2' when true label is 'BAJA+3'\n",
    "    penalty_matrix[1, 3] = -3.0     # Predicted 'BAJA+2' when true label is 'BAJA+4'\n",
    "    penalty_matrix[1, 4] = -3.0     # Predicted 'BAJA+2' when true label is 'CONTINUA'\n",
    "\n",
    "    penalty_matrix[0, 1] = -0.0     # Predicted 'BAJA+1' when true label is 'BAJA+2'\n",
    "    penalty_matrix[0, 2] = -0.0     # Predicted 'BAJA+1' when true label is 'BAJA+3'\n",
    "    penalty_matrix[0, 3] = -0.0     # Predicted 'BAJA+1' when true label is 'BAJA+4'\n",
    "    penalty_matrix[0, 4] = -0.0     # Predicted 'BAJA+1' when true label is 'CONTINUA'\n",
    "\n",
    "    # Add other penalties as per your requirements\n",
    "\n",
    "    # Identify incorrect predictions\n",
    "    incorrect_predictions = ~correct_predictions  # [num_combinations, batch_size, seq_len]\n",
    "\n",
    "    # Get predicted and true indices where predictions are incorrect\n",
    "    predicted_indices = preds[incorrect_predictions]      # [num_incorrect]\n",
    "    true_indices = labels_expanded[incorrect_predictions] # [num_incorrect]\n",
    "\n",
    "    # Gather penalties from the penalty matrix\n",
    "    penalties = penalty_matrix[predicted_indices, true_indices]  # [num_incorrect]\n",
    "\n",
    "    # Assign penalties to the rewards tensor\n",
    "    rewards[incorrect_predictions] += penalties\n",
    "\n",
    "    # ===== Scaling Rewards =====\n",
    "    rewards = rewards * 1  # Scale rewards as per original function\n",
    "\n",
    "    # ===== Apply Mask =====\n",
    "    if mask_expanded is not None:\n",
    "        rewards = rewards * mask_expanded.float()  # Apply mask to rewards\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def threshold_tuning_vectorized_nonmonotonic(model,validation_loader,compute_rewards_ordinal_vectorized,pad_value,\n",
    "    device='cpu',max_combinations_per_batch=1000,val_steps=2):\n",
    "    \n",
    "    import itertools\n",
    "    import torch\n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    # Define threshold ranges\n",
    "    threshold_values = [0.01 + 0.05 * i for i in range(18)]  # From 0.01 to 0.9 with step 0.05\n",
    "    K_minus_1 = model.num_thresholds\n",
    "\n",
    "    # Generate all combinations of thresholds\n",
    "    threshold_combinations = list(itertools.product(threshold_values, repeat=K_minus_1))\n",
    "    num_combinations = len(threshold_combinations)\n",
    "\n",
    "    # Move the model to the device if not already\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize accumulators for total ganancias and confusion matrix components\n",
    "    total_ganancias = torch.zeros(num_combinations, device=device)\n",
    "    tp_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    fp_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    fn_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    # tn_baja2 is not needed for F1-score but can be computed if needed\n",
    "\n",
    "    # Determine the number of chunks\n",
    "    num_chunks = math.ceil(num_combinations / max_combinations_per_batch)\n",
    "\n",
    "    print(f\"num_chunks: {num_chunks}\")\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        # Get the current chunk of threshold combinations\n",
    "        start_idx = chunk_idx * max_combinations_per_batch\n",
    "        end_idx = min((chunk_idx + 1) * max_combinations_per_batch, num_combinations)\n",
    "        current_combinations = threshold_combinations[start_idx:end_idx]\n",
    "        current_num_combinations = len(current_combinations)\n",
    "\n",
    "        # Prepare thresholds tensor\n",
    "        thresholds_tensor = torch.tensor(current_combinations, device=device)  # Shape: [current_num_combinations, K-1]\n",
    "\n",
    "        # Initialize accumulators for the current chunk\n",
    "        total_ganancias_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        tp_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        fp_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        fn_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                X_val_batch, y_val_batch = batch\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "                mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "                mask_float = (~mask).float()\n",
    "\n",
    "                # Remove sequences that are fully padded\n",
    "                all_padded = mask.all(dim=1)\n",
    "                non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "                if non_empty_indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                X_val_batch = X_val_batch[non_empty_indices]\n",
    "                y_val_batch = y_val_batch[non_empty_indices]\n",
    "                mask = mask[non_empty_indices]\n",
    "                mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "                batch_size, seq_len, _ = action_probs.shape\n",
    "\n",
    "                # Compute class predictions for all threshold combinations\n",
    "                # predicted_labels shape: [num_combinations_chunk, batch_size, seq_len]\n",
    "                predicted_labels = get_class_labels_list(action_probs, thresholds=thresholds_tensor)\n",
    "\n",
    "                # Focus on the last 'val_steps' timesteps\n",
    "                predicted_labels_last = predicted_labels[:, :, -val_steps:]  # Shape: [num_combinations_chunk, batch_size, val_steps]\n",
    "                y_val_batch_last = y_val_batch[:, -val_steps:]  # Shape: [batch_size, val_steps]\n",
    "                mask_float_last = mask_float[:, -val_steps:]    # Shape: [batch_size, val_steps]\n",
    "\n",
    "                # Compute rewards for all threshold combinations at once\n",
    "                rewards = compute_rewards_ordinal_vectorized(\n",
    "                    preds=predicted_labels_last,\n",
    "                    labels=y_val_batch_last,\n",
    "                    mask=mask_float_last,\n",
    "                    device=device\n",
    "                )  # Shape: [num_combinations_chunk, batch_size, val_steps]\n",
    "\n",
    "                # Sum rewards over batch and sequence dimensions\n",
    "                total_ganancias_chunk += rewards.sum(dim=(1, 2))  # Shape: [current_num_combinations]\n",
    "\n",
    "                # Collect confusion matrix components for BAJA+2 (class index 1)\n",
    "                # Mask for valid positions (excluding padding and OUT class)\n",
    "                valid_mask = (y_val_batch_last != 5) & (mask_float_last.bool())  # Shape: [batch_size, val_steps]\n",
    "\n",
    "                # Expand valid_mask to match predicted_labels shape\n",
    "                valid_mask_expanded = valid_mask.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "\n",
    "                # Get true labels and predictions where valid\n",
    "                true_labels = y_val_batch_last.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "                preds = predicted_labels_last\n",
    "\n",
    "                # BAJA+2 class index\n",
    "                class_index = 1\n",
    "\n",
    "                # True Positives: predicted BAJA+2 and true label is BAJA+2\n",
    "                tp_mask = (preds == class_index) & (true_labels == class_index) & valid_mask_expanded\n",
    "                tp_baja2_chunk += tp_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # False Positives: predicted BAJA+2 but true label is not BAJA+2\n",
    "                fp_mask = (preds == class_index) & (true_labels != class_index) & valid_mask_expanded\n",
    "                fp_baja2_chunk += fp_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # False Negatives: did not predict BAJA+2 but true label is BAJA+2\n",
    "                fn_mask = (preds != class_index) & (true_labels == class_index) & valid_mask_expanded\n",
    "                fn_baja2_chunk += fn_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # Note: True Negatives are not used in F1-score calculation\n",
    "\n",
    "        # Update accumulators for the entire dataset\n",
    "        total_ganancias[start_idx:end_idx] = total_ganancias_chunk\n",
    "        tp_baja2[start_idx:end_idx] = tp_baja2_chunk\n",
    "        fp_baja2[start_idx:end_idx] = fp_baja2_chunk\n",
    "        fn_baja2[start_idx:end_idx] = fn_baja2_chunk\n",
    "\n",
    "        # Print progress\n",
    "        max_ganancia_chunk = total_ganancias_chunk.max().item()\n",
    "        thresholds_chunk = current_combinations[total_ganancias_chunk.argmax().item()]\n",
    "        print(f\"Processed chunk {chunk_idx + 1}/{num_chunks}, Best Ganancia in chunk: {max_ganancia_chunk} Ts: {thresholds_chunk}\")\n",
    "\n",
    "    # After processing all chunks, we have total_ganancias, tp_baja2, fp_baja2, fn_baja2 for all threshold combinations\n",
    "\n",
    "    # Find the maximum ganancia\n",
    "    max_ganancia = total_ganancias.max().item()\n",
    "\n",
    "    # Find all indices where ganancia equals max_ganancia\n",
    "    best_indices = (total_ganancias == max_ganancia).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Collect F1-scores for BAJA+2 for these indices\n",
    "    precision_baja2 = tp_baja2[best_indices] / (tp_baja2[best_indices] + fp_baja2[best_indices] + 1e-8)\n",
    "    recall_baja2 = tp_baja2[best_indices] / (tp_baja2[best_indices] + fn_baja2[best_indices] + 1e-8)\n",
    "    f1_baja2 = 2 * (precision_baja2 * recall_baja2) / (precision_baja2 + recall_baja2 + 1e-8)\n",
    "\n",
    "    # Ensure f1_baja2 is at least 1D\n",
    "    if f1_baja2.dim() == 0:\n",
    "        f1_baja2 = f1_baja2.unsqueeze(0)\n",
    "\n",
    "    # Find the maximum F1-score for BAJA+2\n",
    "    max_f1_baja2 = f1_baja2.max().item()\n",
    "\n",
    "    # Collect indices with maximum F1-score\n",
    "    indices_with_max_f1 = (f1_baja2 == max_f1_baja2).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Get the corresponding indices from best_indices\n",
    "    best_f1_indices = best_indices[indices_with_max_f1]\n",
    "\n",
    "    # Ensure best_f1_indices is at least 1D\n",
    "    if best_f1_indices.dim() == 0:\n",
    "        best_f1_indices = best_f1_indices.unsqueeze(0)\n",
    "\n",
    "    # Collect corresponding thresholds\n",
    "    best_thresholds_f1 = [threshold_combinations[idx] for idx in best_f1_indices.tolist()]\n",
    "\n",
    "    # If multiple thresholds have the same F1-score, average them\n",
    "    if len(best_thresholds_f1) > 1:\n",
    "        best_thresholds_array = np.array(best_thresholds_f1)\n",
    "        averaged_thresholds = tuple(best_thresholds_array.mean(axis=0))\n",
    "        best_thresholds = averaged_thresholds\n",
    "    else:\n",
    "        best_thresholds = best_thresholds_f1[0]\n",
    "\n",
    "    print(f\"\\nSelected Best Thresholds after maximizing F1-score for BAJA+2:\")\n",
    "    print(f\"Thresholds: {best_thresholds}, F1-score: {max_f1_baja2}, Ganancia: {max_ganancia}\")\n",
    "\n",
    "    # Compute and print the classification report and class counts for the selected thresholds\n",
    "    # This requires one more pass over the validation data with the selected thresholds\n",
    "\n",
    "    all_preds_last = []\n",
    "    all_labels_last = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            X_val_batch, y_val_batch = batch\n",
    "            X_val_batch = X_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device).squeeze(-1)\n",
    "\n",
    "            mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "            mask_float = (~mask).float()\n",
    "\n",
    "            # Remove sequences that are fully padded\n",
    "            all_padded = mask.all(dim=1)\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "            if non_empty_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            X_val_batch = X_val_batch[non_empty_indices]\n",
    "            y_val_batch = y_val_batch[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "            mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "            # Forward pass\n",
    "            action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "            # Use the best thresholds to get predictions\n",
    "            thresholds_tensor = torch.tensor(best_thresholds, device=device)\n",
    "            predicted_labels = get_class_labels_best(action_probs, thresholds=thresholds_tensor)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "            # Focus on the last 'val_steps' timesteps\n",
    "            predicted_labels_last = predicted_labels[:, -val_steps:]  # Shape: [batch_size, val_steps]\n",
    "            y_val_batch_last = y_val_batch[:, -val_steps:]            # Shape: [batch_size, val_steps]\n",
    "\n",
    "            # Flatten and collect predictions and labels\n",
    "            all_preds_last.append(predicted_labels_last.cpu())\n",
    "            all_labels_last.append(y_val_batch_last.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds_last = torch.cat(all_preds_last, dim=0).numpy().flatten()\n",
    "    all_labels_last = torch.cat(all_labels_last, dim=0).numpy().flatten()\n",
    "\n",
    "    # Filter out the OUT class (class value 5)\n",
    "    valid_indices = (all_labels_last != 5)\n",
    "    filtered_preds = all_preds_last[valid_indices]\n",
    "    filtered_labels = all_labels_last[valid_indices]\n",
    "\n",
    "    # Compute and print the classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = [0, 1, 2, 3, 4]\n",
    "    target_names = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA']\n",
    "\n",
    "    print(\"\\nClassification Report for Validation Set with Selected Best Thresholds:\")\n",
    "    print(classification_report(\n",
    "        filtered_labels,\n",
    "        filtered_preds,\n",
    "        labels=class_labels,\n",
    "        target_names=target_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    # Compute and print class counts\n",
    "    from collections import Counter\n",
    "    class_counts = Counter(filtered_preds)\n",
    "    print(\"\\nFinal Class Counts (Predicted for Last Timestep with Selected Best Thresholds):\")\n",
    "    for class_value, class_name in zip(class_labels, target_names):\n",
    "        count = class_counts.get(class_value, 0)\n",
    "        print(f\"{class_name} ({class_value}): {count}\")\n",
    "\n",
    "    return best_thresholds, max_ganancia\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def threshold_tuning_vectorized(\n",
    "    model,\n",
    "    validation_loader,\n",
    "    compute_rewards_ordinal_vectorized,\n",
    "    pad_value,\n",
    "    device='cpu',\n",
    "    max_combinations_per_batch=1000,\n",
    "    val_steps=val_steps\n",
    "):\n",
    "    import itertools\n",
    "    import torch\n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    # Define threshold ranges\n",
    "    threshold_values = [0.01 + 0.03 * i for i in range(25)]  # From 0.01 to 0.9 with step 0.05\n",
    "    K_minus_1 = model.num_thresholds\n",
    "\n",
    "    # Generate all combinations of thresholds with non-decreasing order\n",
    "    threshold_combinations = list(itertools.combinations_with_replacement(threshold_values, K_minus_1))\n",
    "    num_combinations = len(threshold_combinations)\n",
    "\n",
    "    # Move the model to the device if not already\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize accumulators for total ganancias and confusion matrix components\n",
    "    total_ganancias = torch.zeros(num_combinations, device=device)\n",
    "    tp_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    fp_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    fn_baja2 = torch.zeros(num_combinations, device=device)\n",
    "    # tn_baja2 is not needed for F1-score but can be computed if needed\n",
    "\n",
    "    # Determine the number of chunks\n",
    "    num_chunks = math.ceil(num_combinations / max_combinations_per_batch)\n",
    "\n",
    "    #if print_grid_chunks:\n",
    "    #    print(f\"num_chunks: {num_chunks}\")\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        # Get the current chunk of threshold combinations\n",
    "        start_idx = chunk_idx * max_combinations_per_batch\n",
    "        end_idx = min((chunk_idx + 1) * max_combinations_per_batch, num_combinations)\n",
    "        current_combinations = threshold_combinations[start_idx:end_idx]\n",
    "        current_num_combinations = len(current_combinations)\n",
    "\n",
    "        # Prepare thresholds tensor\n",
    "        thresholds_tensor = torch.tensor(current_combinations, device=device)  # Shape: [current_num_combinations, K-1]\n",
    "\n",
    "        # Initialize accumulators for the current chunk\n",
    "        total_ganancias_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        tp_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        fp_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "        fn_baja2_chunk = torch.zeros(current_num_combinations, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                X_val_batch, y_val_batch = batch\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "                mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "                mask_float = (~mask).float()\n",
    "\n",
    "                # Remove sequences that are fully padded\n",
    "                all_padded = mask.all(dim=1)\n",
    "                non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "                if non_empty_indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                X_val_batch = X_val_batch[non_empty_indices]\n",
    "                y_val_batch = y_val_batch[non_empty_indices]\n",
    "                mask = mask[non_empty_indices]\n",
    "                mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "                batch_size, seq_len, _ = action_probs.shape\n",
    "\n",
    "                # Compute class predictions for all threshold combinations\n",
    "                # predicted_labels shape: [num_combinations_chunk, batch_size, seq_len]\n",
    "                predicted_labels = get_class_labels_list(action_probs, thresholds=thresholds_tensor)\n",
    "\n",
    "                # Focus on the last 'val_steps' timesteps\n",
    "                predicted_labels_last = predicted_labels[:, :, -val_steps:]  # Shape: [num_combinations_chunk, batch_size, val_steps]\n",
    "                y_val_batch_last = y_val_batch[:, -val_steps:]  # Shape: [batch_size, val_steps]\n",
    "                mask_float_last = mask_float[:, -val_steps:]    # Shape: [batch_size, val_steps]\n",
    "\n",
    "                # Compute rewards for all threshold combinations at once\n",
    "                rewards = compute_rewards_ordinal_vectorized(\n",
    "                    preds=predicted_labels_last,\n",
    "                    labels=y_val_batch_last,\n",
    "                    mask=mask_float_last,\n",
    "                    device=device\n",
    "                )  # Shape: [num_combinations_chunk, batch_size, val_steps]\n",
    "\n",
    "                # Sum rewards over batch and sequence dimensions\n",
    "                total_ganancias_chunk += rewards.sum(dim=(1, 2))  # Shape: [current_num_combinations]\n",
    "\n",
    "                # Collect confusion matrix components for BAJA+2 (class index 1)\n",
    "                # Mask for valid positions (excluding padding and OUT class)\n",
    "                valid_mask = (y_val_batch_last != 5) & (mask_float_last.bool())  # Shape: [batch_size, val_steps]\n",
    "\n",
    "                # Expand valid_mask to match predicted_labels shape\n",
    "                valid_mask_expanded = valid_mask.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "\n",
    "                # Get true labels and predictions where valid\n",
    "                true_labels = y_val_batch_last.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "                preds = predicted_labels_last\n",
    "\n",
    "                # BAJA+2 class index\n",
    "                class_index = 1\n",
    "\n",
    "                # True Positives: predicted BAJA+2 and true label is BAJA+2\n",
    "                tp_mask = (preds == class_index) & (true_labels == class_index) & valid_mask_expanded\n",
    "                tp_baja2_chunk += tp_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # False Positives: predicted BAJA+2 but true label is not BAJA+2\n",
    "                fp_mask = (preds == class_index) & (true_labels != class_index) & valid_mask_expanded\n",
    "                fp_baja2_chunk += fp_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # False Negatives: did not predict BAJA+2 but true label is BAJA+2\n",
    "                fn_mask = (preds != class_index) & (true_labels == class_index) & valid_mask_expanded\n",
    "                fn_baja2_chunk += fn_mask.sum(dim=(1, 2))\n",
    "\n",
    "                # Note: True Negatives are not used in F1-score calculation\n",
    "\n",
    "        # Update accumulators for the entire dataset\n",
    "        total_ganancias[start_idx:end_idx] = total_ganancias_chunk\n",
    "        tp_baja2[start_idx:end_idx] = tp_baja2_chunk\n",
    "        fp_baja2[start_idx:end_idx] = fp_baja2_chunk\n",
    "        fn_baja2[start_idx:end_idx] = fn_baja2_chunk\n",
    "\n",
    "        # Print progress\n",
    "        max_ganancia_chunk = total_ganancias_chunk.max().item()\n",
    "        thresholds_chunk = current_combinations[total_ganancias_chunk.argmax().item()]\n",
    "\n",
    "\n",
    "        if print_grid_chunks:\n",
    "            print(f\"Processed chunk {chunk_idx + 1}/{num_chunks}, Best Ganancia in chunk: {max_ganancia_chunk} Ts: {thresholds_chunk}\")\n",
    "\n",
    "    # After processing all chunks, we have total_ganancias, tp_baja2, fp_baja2, fn_baja2 for all threshold combinations\n",
    "\n",
    "    # Find the maximum ganancia\n",
    "    max_ganancia = total_ganancias.max().item()\n",
    "\n",
    "    # Find all indices where ganancia equals max_ganancia\n",
    "    best_indices = (total_ganancias == max_ganancia).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Collect F1-scores for BAJA+2 for these indices\n",
    "    precision_baja2 = tp_baja2[best_indices] / (tp_baja2[best_indices] + fp_baja2[best_indices] + 1e-8)\n",
    "    recall_baja2 = tp_baja2[best_indices] / (tp_baja2[best_indices] + fn_baja2[best_indices] + 1e-8)\n",
    "    f1_baja2 = 2 * (precision_baja2 * recall_baja2) / (precision_baja2 + recall_baja2 + 1e-8)\n",
    "\n",
    "    # Ensure f1_baja2 is at least 1D\n",
    "    if f1_baja2.dim() == 0:\n",
    "        f1_baja2 = f1_baja2.unsqueeze(0)\n",
    "\n",
    "    # Find the maximum F1-score for BAJA+2\n",
    "    max_f1_baja2 = f1_baja2.max().item()\n",
    "\n",
    "    # Collect indices with maximum F1-score\n",
    "    indices_with_max_f1 = (f1_baja2 == max_f1_baja2).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Get the corresponding indices from best_indices\n",
    "    best_f1_indices = best_indices[indices_with_max_f1]\n",
    "\n",
    "    # Ensure best_f1_indices is at least 1D\n",
    "    if best_f1_indices.dim() == 0:\n",
    "        best_f1_indices = best_f1_indices.unsqueeze(0)\n",
    "\n",
    "    # Collect corresponding thresholds\n",
    "    best_thresholds_f1 = [threshold_combinations[idx] for idx in best_f1_indices.tolist()]\n",
    "\n",
    "    # If multiple thresholds have the same F1-score, average them\n",
    "    if len(best_thresholds_f1) > 1:\n",
    "        best_thresholds_array = np.array(best_thresholds_f1)\n",
    "        averaged_thresholds = tuple(best_thresholds_array.mean(axis=0))\n",
    "        best_thresholds = averaged_thresholds\n",
    "    else:\n",
    "        best_thresholds = best_thresholds_f1[0]\n",
    "\n",
    "    print(f\"\\nSelected Best Thresholds after maximizing F1-score for BAJA+2:\")\n",
    "    print(f\"Thresholds: {best_thresholds}, F1-score: {max_f1_baja2}, Ganancia: {max_ganancia}\")\n",
    "\n",
    "    # Compute and print the classification report and class counts for the selected thresholds\n",
    "    # This requires one more pass over the validation data with the selected thresholds\n",
    "\n",
    "    all_preds_last = []\n",
    "    all_labels_last = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            X_val_batch, y_val_batch = batch\n",
    "            X_val_batch = X_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device).squeeze(-1)\n",
    "\n",
    "            mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "            mask_float = (~mask).float()\n",
    "\n",
    "            # Remove sequences that are fully padded\n",
    "            all_padded = mask.all(dim=1)\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "            if non_empty_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            X_val_batch = X_val_batch[non_empty_indices]\n",
    "            y_val_batch = y_val_batch[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "            mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "            # Forward pass\n",
    "            action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "            # Use the best thresholds to get predictions\n",
    "            thresholds_tensor = torch.tensor(best_thresholds, device=device)\n",
    "            predicted_labels = get_class_labels_best(action_probs, thresholds=thresholds_tensor)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "            # Focus on the last 'val_steps' timesteps\n",
    "            predicted_labels_last = predicted_labels[:, -val_steps:]  # Shape: [batch_size, val_steps]\n",
    "            y_val_batch_last = y_val_batch[:, -val_steps:]            # Shape: [batch_size, val_steps]\n",
    "\n",
    "            # Flatten and collect predictions and labels\n",
    "            all_preds_last.append(predicted_labels_last.cpu())\n",
    "            all_labels_last.append(y_val_batch_last.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds_last = torch.cat(all_preds_last, dim=0).numpy().flatten()\n",
    "    all_labels_last = torch.cat(all_labels_last, dim=0).numpy().flatten()\n",
    "\n",
    "    # Filter out the OUT class (class value 5)\n",
    "    valid_indices = (all_labels_last != 5)\n",
    "    filtered_preds = all_preds_last[valid_indices]\n",
    "    filtered_labels = all_labels_last[valid_indices]\n",
    "\n",
    "    # Compute and print the classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = [0, 1, 2, 3, 4]\n",
    "    target_names = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA']\n",
    "\n",
    "    if print_grid_report:\n",
    "        print(\"\\nClassification Report for Validation Set with Selected Best Thresholds:\")\n",
    "        print(classification_report(\n",
    "            filtered_labels,\n",
    "            filtered_preds,\n",
    "            labels=class_labels,\n",
    "            target_names=target_names,\n",
    "            zero_division=0\n",
    "        ))\n",
    "\n",
    "    if print_grid_count:\n",
    "        # Compute and print class counts\n",
    "        from collections import Counter\n",
    "        class_counts = Counter(filtered_preds)\n",
    "        print(\"\\nFinal Class Counts (Predicted for Last Timestep with Selected Best Thresholds):\")\n",
    "        for class_value, class_name in zip(class_labels, target_names):\n",
    "            count = class_counts.get(class_value, 0)\n",
    "            print(f\"{class_name} ({class_value}): {count}\")\n",
    "\n",
    "    return best_thresholds, max_ganancia\n",
    "\n",
    "\n",
    "\n",
    "def threshold_tuning_vectorized_whole_val(\n",
    "    model,\n",
    "    validation_loader,\n",
    "    compute_rewards_ordinal_vectorized,\n",
    "    pad_value,\n",
    "    device='cpu',\n",
    "    max_combinations_per_batch=1000,\n",
    "    val_steps=val_steps\n",
    "):\n",
    "    import itertools\n",
    "    import torch\n",
    "    import math\n",
    "    import numpy as np\n",
    "\n",
    "    # Define threshold ranges\n",
    "    threshold_values = [0.01 + 0.03 * i for i in range(30)]  # From 0.01 to 0.935 with step 0.025\n",
    "    K_minus_1 = model.num_thresholds\n",
    "\n",
    "    # Generate all combinations of thresholds with non-decreasing order\n",
    "    threshold_combinations = list(itertools.combinations_with_replacement(threshold_values, K_minus_1))\n",
    "    num_combinations = len(threshold_combinations)\n",
    "\n",
    "    # Move the model to the device if not already\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize accumulators for total ganancias and confusion matrix components\n",
    "    total_ganancias_excl_last = torch.zeros(num_combinations, device=device)\n",
    "    total_ganancias_last = torch.zeros(num_combinations, device=device)\n",
    "    tp_baja2_last = torch.zeros(num_combinations, device=device)\n",
    "    fp_baja2_last = torch.zeros(num_combinations, device=device)\n",
    "    fn_baja2_last = torch.zeros(num_combinations, device=device)\n",
    "    # tn_baja2_last is not needed for F1-score but can be computed if needed\n",
    "\n",
    "    # Determine the number of chunks\n",
    "    num_chunks = math.ceil(num_combinations / max_combinations_per_batch)\n",
    "\n",
    "    print(f\"num_chunks: {num_chunks}\")\n",
    "\n",
    "    for chunk_idx in range(num_chunks):\n",
    "        # Get the current chunk of threshold combinations\n",
    "        start_idx = chunk_idx * max_combinations_per_batch\n",
    "        end_idx = min((chunk_idx + 1) * max_combinations_per_batch, num_combinations)\n",
    "        current_combinations = threshold_combinations[start_idx:end_idx]\n",
    "        current_num_combinations = len(current_combinations)\n",
    "\n",
    "        # Prepare thresholds tensor\n",
    "        thresholds_tensor = torch.tensor(current_combinations, device=device)  # Shape: [current_num_combinations, K-1]\n",
    "\n",
    "        # Initialize accumulators for the current chunk\n",
    "        total_ganancias_chunk_excl_last = torch.zeros(current_num_combinations, device=device)\n",
    "        total_ganancias_chunk_last = torch.zeros(current_num_combinations, device=device)\n",
    "        tp_baja2_chunk_last = torch.zeros(current_num_combinations, device=device)\n",
    "        fp_baja2_chunk_last = torch.zeros(current_num_combinations, device=device)\n",
    "        fn_baja2_chunk_last = torch.zeros(current_num_combinations, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                X_val_batch, y_val_batch = batch\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "                mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "                mask_float = (~mask).float()\n",
    "\n",
    "                # Remove sequences that are fully padded\n",
    "                all_padded = mask.all(dim=1)\n",
    "                non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "                if non_empty_indices.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                X_val_batch = X_val_batch[non_empty_indices]\n",
    "                y_val_batch = y_val_batch[non_empty_indices]\n",
    "                mask = mask[non_empty_indices]\n",
    "                mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "                batch_size, seq_len, _ = action_probs.shape\n",
    "\n",
    "                # Compute class predictions for all threshold combinations\n",
    "                # predicted_labels shape: [num_combinations_chunk, batch_size, seq_len]\n",
    "                predicted_labels = get_class_labels_list(action_probs, thresholds=thresholds_tensor)\n",
    "\n",
    "                # Exclude the last 'val_steps' timesteps for threshold tuning\n",
    "                if val_steps > 0:\n",
    "                    predicted_labels_excl_last = predicted_labels[:, :, :-val_steps]\n",
    "                    y_val_batch_excl_last = y_val_batch[:, :-val_steps]\n",
    "                    mask_float_excl_last = mask_float[:, :-val_steps]\n",
    "                else:\n",
    "                    predicted_labels_excl_last = predicted_labels\n",
    "                    y_val_batch_excl_last = y_val_batch\n",
    "                    mask_float_excl_last = mask_float\n",
    "\n",
    "                # Compute rewards for all threshold combinations at once (excluding last val_steps)\n",
    "                rewards_excl_last = compute_rewards_ordinal_vectorized(\n",
    "                    preds=predicted_labels_excl_last,\n",
    "                    labels=y_val_batch_excl_last,\n",
    "                    mask=mask_float_excl_last,\n",
    "                    device=device\n",
    "                )  # Shape: [current_num_combinations, batch_size, seq_len - val_steps]\n",
    "\n",
    "                # Sum rewards over batch and sequence dimensions\n",
    "                total_ganancias_chunk_excl_last += rewards_excl_last.sum(dim=(1, 2))  # Shape: [current_num_combinations]\n",
    "\n",
    "                # Compute rewards over last val_steps for reporting\n",
    "                predicted_labels_last = predicted_labels[:, :, -val_steps:]  # Shape: [num_combinations_chunk, batch_size, val_steps]\n",
    "                y_val_batch_last = y_val_batch[:, -val_steps:]               # Shape: [batch_size, val_steps]\n",
    "                mask_float_last = mask_float[:, -val_steps:]                 # Shape: [batch_size, val_steps]\n",
    "\n",
    "                rewards_last = compute_rewards_ordinal_vectorized(\n",
    "                    preds=predicted_labels_last,\n",
    "                    labels=y_val_batch_last,\n",
    "                    mask=mask_float_last,\n",
    "                    device=device\n",
    "                )  # Shape: [current_num_combinations, batch_size, val_steps]\n",
    "\n",
    "                total_ganancias_chunk_last += rewards_last.sum(dim=(1, 2))  # Shape: [current_num_combinations]\n",
    "\n",
    "                # Collect confusion matrix components for BAJA+2 (class index 1) over last val_steps\n",
    "                # Mask for valid positions (excluding padding and OUT class)\n",
    "                valid_mask_last = (y_val_batch_last != 5) & (mask_float_last.bool())  # Shape: [batch_size, val_steps]\n",
    "\n",
    "                # Expand valid_mask to match predicted_labels shape\n",
    "                valid_mask_expanded_last = valid_mask_last.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "\n",
    "                # Get true labels and predictions where valid\n",
    "                true_labels_last = y_val_batch_last.unsqueeze(0).expand(current_num_combinations, -1, -1)\n",
    "                preds_last = predicted_labels_last\n",
    "\n",
    "                # BAJA+2 class index\n",
    "                class_index = 1\n",
    "\n",
    "                # True Positives: predicted BAJA+2 and true label is BAJA+2\n",
    "                tp_mask_last = (preds_last == class_index) & (true_labels_last == class_index) & valid_mask_expanded_last\n",
    "                tp_baja2_chunk_last += tp_mask_last.sum(dim=(1, 2))\n",
    "\n",
    "                # False Positives: predicted BAJA+2 but true label is not BAJA+2\n",
    "                fp_mask_last = (preds_last == class_index) & (true_labels_last != class_index) & valid_mask_expanded_last\n",
    "                fp_baja2_chunk_last += fp_mask_last.sum(dim=(1, 2))\n",
    "\n",
    "                # False Negatives: did not predict BAJA+2 but true label is BAJA+2\n",
    "                fn_mask_last = (preds_last != class_index) & (true_labels_last == class_index) & valid_mask_expanded_last\n",
    "                fn_baja2_chunk_last += fn_mask_last.sum(dim=(1, 2))\n",
    "\n",
    "                # Note: True Negatives are not used in F1-score calculation\n",
    "\n",
    "        # Update accumulators for the entire dataset\n",
    "        total_ganancias_excl_last[start_idx:end_idx] = total_ganancias_chunk_excl_last\n",
    "        total_ganancias_last[start_idx:end_idx] = total_ganancias_chunk_last\n",
    "        tp_baja2_last[start_idx:end_idx] = tp_baja2_chunk_last\n",
    "        fp_baja2_last[start_idx:end_idx] = fp_baja2_chunk_last\n",
    "        fn_baja2_last[start_idx:end_idx] = fn_baja2_chunk_last\n",
    "\n",
    "        # Print progress using ganancia over last val_steps\n",
    "        max_ganancia_chunk_last = total_ganancias_chunk_last.max().item()\n",
    "        thresholds_chunk = current_combinations[total_ganancias_chunk_last.argmax().item()]\n",
    "        print(f\"Processed chunk {chunk_idx + 1}/{num_chunks}, Best Ganancia in chunk (last {val_steps} steps): {max_ganancia_chunk_last} Ts: {thresholds_chunk}\")\n",
    "\n",
    "    # After processing all chunks, we have total_ganancias_excl_last for threshold tuning\n",
    "    # and total_ganancias_last for reporting\n",
    "\n",
    "    # Find the maximum ganancia over earlier timesteps (excluding last val_steps)\n",
    "    max_ganancia = total_ganancias_excl_last.max().item()\n",
    "\n",
    "    # Find all indices where ganancia equals max_ganancia\n",
    "    best_indices = (total_ganancias_excl_last == max_ganancia).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Collect F1-scores for BAJA+2 over last val_steps for these indices\n",
    "    precision_baja2 = tp_baja2_last[best_indices] / (tp_baja2_last[best_indices] + fp_baja2_last[best_indices] + 1e-8)\n",
    "    recall_baja2 = tp_baja2_last[best_indices] / (tp_baja2_last[best_indices] + fn_baja2_last[best_indices] + 1e-8)\n",
    "    f1_baja2 = 2 * (precision_baja2 * recall_baja2) / (precision_baja2 + recall_baja2 + 1e-8)\n",
    "\n",
    "    # Ensure f1_baja2 is at least 1D\n",
    "    if f1_baja2.dim() == 0:\n",
    "        f1_baja2 = f1_baja2.unsqueeze(0)\n",
    "\n",
    "    # Find the maximum F1-score for BAJA+2 over last val_steps\n",
    "    max_f1_baja2 = f1_baja2.max().item()\n",
    "\n",
    "    # Collect indices with maximum F1-score\n",
    "    indices_with_max_f1 = (f1_baja2 == max_f1_baja2).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Get the corresponding indices from best_indices\n",
    "    best_f1_indices = best_indices[indices_with_max_f1]\n",
    "\n",
    "    # Ensure best_f1_indices is at least 1D\n",
    "    if best_f1_indices.dim() == 0:\n",
    "        best_f1_indices = best_f1_indices.unsqueeze(0)\n",
    "\n",
    "    # Collect corresponding thresholds\n",
    "    best_thresholds_f1 = [threshold_combinations[idx] for idx in best_f1_indices.tolist()]\n",
    "\n",
    "    # If multiple thresholds have the same F1-score, average them\n",
    "    if len(best_thresholds_f1) > 1:\n",
    "        best_thresholds_array = np.array(best_thresholds_f1)\n",
    "        averaged_thresholds = tuple(best_thresholds_array.mean(axis=0))\n",
    "        best_thresholds = averaged_thresholds\n",
    "    else:\n",
    "        best_thresholds = best_thresholds_f1[0]\n",
    "\n",
    "    # Report the maximum ganancia over last val_steps\n",
    "    max_ganancia_last = total_ganancias_last[best_f1_indices].max().item()\n",
    "\n",
    "    print(f\"\\nSelected Best Thresholds after maximizing F1-score for BAJA+2:\")\n",
    "    print(f\"Thresholds: {best_thresholds}, F1-score: {max_f1_baja2}, Ganancia over last {val_steps} steps: {max_ganancia_last}\")\n",
    "\n",
    "    # Compute and print the classification report and class counts for the selected thresholds\n",
    "    # This requires one more pass over the validation data with the selected thresholds\n",
    "\n",
    "    all_preds_last = []\n",
    "    all_labels_last = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "            X_val_batch, y_val_batch = batch\n",
    "            X_val_batch = X_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device).squeeze(-1)\n",
    "\n",
    "            mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "            mask_float = (~mask).float()\n",
    "\n",
    "            # Remove sequences that are fully padded\n",
    "            all_padded = mask.all(dim=1)\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "            if non_empty_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            X_val_batch = X_val_batch[non_empty_indices]\n",
    "            y_val_batch = y_val_batch[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "            mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "            # Forward pass\n",
    "            action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "            # Use the best thresholds to get predictions\n",
    "            thresholds_tensor = torch.tensor(best_thresholds, device=device)\n",
    "            predicted_labels = get_class_labels_best(action_probs, thresholds=thresholds_tensor)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "            # Focus on the last 'val_steps' timesteps\n",
    "            predicted_labels_last = predicted_labels[:, -val_steps:]  # Shape: [batch_size, val_steps]\n",
    "            y_val_batch_last = y_val_batch[:, -val_steps:]            # Shape: [batch_size, val_steps]\n",
    "\n",
    "            # Flatten and collect predictions and labels\n",
    "            all_preds_last.append(predicted_labels_last.cpu())\n",
    "            all_labels_last.append(y_val_batch_last.cpu())\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    all_preds_last = torch.cat(all_preds_last, dim=0).numpy().flatten()\n",
    "    all_labels_last = torch.cat(all_labels_last, dim=0).numpy().flatten()\n",
    "\n",
    "    # Filter out the OUT class (class value 5)\n",
    "    valid_indices = (all_labels_last != 5)\n",
    "    filtered_preds = all_preds_last[valid_indices]\n",
    "    filtered_labels = all_labels_last[valid_indices]\n",
    "\n",
    "    # Compute and print the classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    class_labels = [0, 1, 2, 3, 4]\n",
    "    target_names = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA']\n",
    "\n",
    "    print(f\"\\nClassification Report for Validation Set (Last {val_steps} Steps) with Selected Best Thresholds:\")\n",
    "    print(classification_report(\n",
    "        filtered_labels,\n",
    "        filtered_preds,\n",
    "        labels=class_labels,\n",
    "        target_names=target_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    # Compute and print class counts\n",
    "    from collections import Counter\n",
    "    class_counts = Counter(filtered_preds)\n",
    "    print(f\"\\nFinal Class Counts (Predicted for Last {val_steps} Steps with Selected Best Thresholds):\")\n",
    "    for class_value, class_name in zip(class_labels, target_names):\n",
    "        count = class_counts.get(class_value, 0)\n",
    "        print(f\"{class_name} ({class_value}): {count}\")\n",
    "\n",
    "    return best_thresholds, max_ganancia_last\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8897c166-6c19-4eba-b03b-ba2f72278800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels_list(action_probs, thresholds):\n",
    "    \"\"\"\n",
    "    Convert threshold probabilities to class labels using multiple threshold combinations.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): Probabilities for each ordinal threshold.\n",
    "                                     Shape: [batch_size, seq_len, K-1]\n",
    "        thresholds (torch.Tensor): Threshold values for each class boundary.\n",
    "                                   Shape: [num_combinations, K-1]\n",
    "\n",
    "    Returns:\n",
    "        class_labels (torch.Tensor): Predicted class indices.\n",
    "                                     Shape: [num_combinations, batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Ensure thresholds are on the same device\n",
    "    thresholds = thresholds.to(action_probs.device)\n",
    "\n",
    "    #print(f\"action_probs shape: {action_probs.shape}\")\n",
    "    #print(f\"thresholds shape before reshape: {thresholds.shape}\")\n",
    "\n",
    "    # Reshape thresholds for broadcasting\n",
    "    if thresholds.dim() == 2:\n",
    "        # thresholds shape is [num_combinations, K-1]\n",
    "        thresholds = thresholds.view(-1, 1, 1, thresholds.size(-1))  # Shape: [num_combinations, 1, 1, K-1]\n",
    "    else:\n",
    "        # thresholds shape is [K-1]\n",
    "        thresholds = thresholds.view(1, 1, 1, -1)  # Shape: [1, 1, 1, K-1]\n",
    "\n",
    "    #print(f\"thresholds shape after reshape: {thresholds.shape}\")\n",
    "\n",
    "    # Expand action_probs for broadcasting\n",
    "    action_probs_expanded = action_probs.unsqueeze(0)  # Shape: [1, batch_size, seq_len, K-1]\n",
    "    #print(f\"action_probs_expanded shape: {action_probs_expanded.shape}\")\n",
    "\n",
    "    # Compare probabilities against thresholds\n",
    "    exceeded = (action_probs_expanded > thresholds).int()  # Shape: [num_combinations, batch_size, seq_len, K-1]\n",
    "\n",
    "    # Sum to get class labels\n",
    "    class_labels = exceeded.sum(dim=-1)  # Shape: [num_combinations, batch_size, seq_len]\n",
    "\n",
    "    # Clamp class labels\n",
    "    class_labels = torch.clamp(class_labels, 0, action_probs.size(-1))\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d2bba5-c20c-4bab-a879-4f4afe5f8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(sequence_array, label_array, train_steps, val_steps, test_steps, split_by_clients=False, client_split_ratios=(0.7, 0.15, 0.15)):\n",
    "    num_timesteps = sequence_array.shape[1]  # Total number of time steps\n",
    "    \n",
    "    if split_by_clients:\n",
    "        # Split based on clients (split the rows first)\n",
    "        train_ratio, val_ratio, test_ratio = client_split_ratios\n",
    "        \n",
    "        assert np.isclose(train_ratio + val_ratio + test_ratio, 1.0), \"Client split ratios must sum to 1.\"\n",
    "        \n",
    "        # Adjust for test_ratio being zero\n",
    "        if test_ratio == 0:\n",
    "            # Split clients into train and validation only\n",
    "            X_train_clients, X_val_clients, y_train_clients, y_val_clients = train_test_split(\n",
    "                sequence_array, label_array, test_size=val_ratio, random_state=42)\n",
    "            X_test_clients, y_test_clients = np.array([]), np.array([])  # Empty arrays for test\n",
    "        else:\n",
    "            # Split clients into train, validation, and test sets\n",
    "            X_train_clients, X_temp, y_train_clients, y_temp = train_test_split(sequence_array, label_array, test_size=(1 - train_ratio), random_state=42)\n",
    "            X_val_clients, X_test_clients, y_val_clients, y_test_clients = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
    "        \n",
    "        # Now split each client set by time steps\n",
    "        X_train, y_train = _split_by_time_steps(X_train_clients, y_train_clients, train_steps, val_steps, test_steps, 'train')\n",
    "        X_val, y_val = _split_by_time_steps(X_val_clients, y_val_clients, train_steps, val_steps, test_steps, 'val')\n",
    "        \n",
    "        if test_ratio > 0:\n",
    "            X_test, y_test = _split_by_time_steps(X_test_clients, y_test_clients, train_steps, val_steps, test_steps, 'test')\n",
    "        else:\n",
    "            X_test, y_test = np.array([]), np.array([])  # Empty arrays for test\n",
    "    else:\n",
    "        # If not splitting by clients, just split by time steps on the whole dataset\n",
    "        X_train, y_train = _split_by_time_steps(sequence_array, label_array, train_steps, val_steps, test_steps, 'train')\n",
    "        X_val, y_val = _split_by_time_steps(sequence_array, label_array, train_steps, val_steps, test_steps, 'val')\n",
    "        \n",
    "        if test_steps > 0:\n",
    "            X_test, y_test = _split_by_time_steps(sequence_array, label_array, train_steps, val_steps, test_steps, 'test')\n",
    "        else:\n",
    "            X_test, y_test = np.array([]), np.array([])  # Empty arrays for test\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def _split_by_time_steps(sequence_array, label_array, train_steps, val_steps, test_steps, split_type):\n",
    "    num_timesteps = sequence_array.shape[1]\n",
    "    \n",
    "    assert train_steps + val_steps + test_steps <= num_timesteps, f\"Not enough steps for {split_type}.\"\n",
    "\n",
    "    if split_type == 'train':\n",
    "        X_split = sequence_array[:, :train_steps, :]\n",
    "        y_split = label_array[:, :train_steps]\n",
    "    \n",
    "    elif split_type == 'val':\n",
    "        # Validation contains all train steps plus validation steps\n",
    "        X_split = sequence_array[:, :train_steps + val_steps, :]  # Combine train and val steps\n",
    "        y_split = label_array[:, :train_steps + val_steps]\n",
    "    \n",
    "    elif split_type == 'test':\n",
    "        X_split = sequence_array[:, train_steps + val_steps:train_steps + val_steps + test_steps, :]\n",
    "        y_split = label_array[:, train_steps + val_steps:train_steps + val_steps + test_steps]\n",
    "\n",
    "    return X_split, y_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a97973e-4ca7-47be-9d3c-2fb877e00343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(input_seq, pad_token=0):\n",
    "    # Returns True for padding positions, False for valid positions\n",
    "    mask = (input_seq == pad_token).all(dim=-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bde947c6-2501-4d71-b86c-12579b2f571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class values and their corresponding names\n",
    "CLASS_VALUES = np.array([1, 2, 3, 4, 5, 69])\n",
    "TARGET_NAMES = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA', 'OUT']\n",
    "\n",
    "#-----------------ORDINAL CLASSIFIER----------------------\n",
    "\n",
    "def get_class_labels(action_probs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Convert threshold probabilities to class labels.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): Probabilities for each ordinal threshold.\n",
    "                                     Shape: [batch_size, seq_len, K-1]\n",
    "        threshold (float): Probability threshold to determine class boundaries.\n",
    "\n",
    "    Returns:\n",
    "        class_labels (torch.Tensor): Predicted class indices.\n",
    "                                     Shape: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Compare probabilities against the threshold\n",
    "    exceeded = (action_probs > threshold).int()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "    # Sum the number of exceeded thresholds to get class labels\n",
    "    class_labels = exceeded.sum(dim=-1)  # [batch_size, seq_len]\n",
    "\n",
    "    # Clamp class_labels to be within [0, K-1]\n",
    "    class_labels = torch.clamp(class_labels, 0, action_probs.size(-1))\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "def get_class_labels_best(action_probs, thresholds):\n",
    "    \"\"\"\n",
    "    Convert threshold probabilities to class labels using thresholds per class boundary.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): Probabilities for each ordinal threshold.\n",
    "                                     Shape: [batch_size, seq_len, K-1]\n",
    "        thresholds (float or list or torch.Tensor): Thresholds for each class boundary.\n",
    "                                                    Shape: [K-1]\n",
    "\n",
    "    Returns:\n",
    "        class_labels (torch.Tensor): Predicted class indices.\n",
    "                                     Shape: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    # Ensure thresholds is a torch tensor\n",
    "    if not isinstance(thresholds, torch.Tensor):\n",
    "        thresholds = torch.tensor(thresholds, device=action_probs.device, dtype=action_probs.dtype)\n",
    "    else:\n",
    "        thresholds = thresholds.to(action_probs.device).type_as(action_probs)\n",
    "\n",
    "    # Reshape thresholds for broadcasting\n",
    "    thresholds = thresholds.view(1, 1, -1)  # Shape: [1, 1, K-1]\n",
    "\n",
    "    # Compare probabilities against thresholds\n",
    "    exceeded = (action_probs > thresholds).int()  # Shape: [batch_size, seq_len, K-1]\n",
    "\n",
    "    # Sum the number of thresholds exceeded to get class labels\n",
    "    class_labels = exceeded.sum(dim=-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "    # Clamp class_labels to be within [0, K]\n",
    "    class_labels = torch.clamp(class_labels, 0, action_probs.size(-1))\n",
    "\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_best_threshold_ganancia(preds, labels, baja_2_index=1):\n",
    "    \"\"\"\n",
    "    Compute ganancia based on predicted class indices.\n",
    "\n",
    "    Args:\n",
    "        preds (np.ndarray): Predicted class indices. Shape: [num_samples]\n",
    "        labels (np.ndarray): Ground truth class indices. Shape: [num_samples]\n",
    "        baja_2_index (int): The class index for BAJA+2 (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        ganancia (float): Calculated ganancia based on predictions.\n",
    "        best_ganancia_argmax (float): Set equal to ganancia for simplicity.\n",
    "        best_baja_1_threshold (None): Not applicable.\n",
    "        best_baja_2_threshold (None): Not applicable.\n",
    "        best_continua_threshold (None): Not applicable.\n",
    "        preds (np.ndarray): Predicted class indices.\n",
    "    \"\"\"\n",
    "    # Calculate ganancia based on BAJA+2 predictions\n",
    "    ganancia = calculate_ganancia(preds, labels, baja_2_index)\n",
    "\n",
    "    # For ordinal classification, argmax isn't directly applicable as predictions are class indices\n",
    "    best_ganancia_argmax = ganancia\n",
    "\n",
    "    return ganancia, best_ganancia_argmax, None, None, None, preds\n",
    "\n",
    "\n",
    "def calculate_ganancia(preds, labels, baja_2_index=1):\n",
    "    \"\"\"\n",
    "    Calculate ganancia based on BAJA+2 predictions.\n",
    "\n",
    "    Args:\n",
    "        preds (np.ndarray): Predicted class indices. Shape: [num_samples]\n",
    "        labels (np.ndarray): Ground truth class indices. Shape: [num_samples]\n",
    "        baja_2_index (int): The class index for BAJA+2 (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        ganancia (float): Calculated ganancia.\n",
    "    \"\"\"\n",
    "    # Define reward for correct BAJA+2 and penalty for incorrect BAJA+2\n",
    "    reward_baja_2 = 117\n",
    "    penalty_baja_2 = -3\n",
    "\n",
    "    # Calculate the number of correct BAJA+2 predictions\n",
    "    correct_baja_2 = np.sum((preds == baja_2_index) & (labels == baja_2_index))\n",
    "\n",
    "    # Calculate the number of incorrect BAJA+2 predictions\n",
    "    incorrect_baja_2 = np.sum((preds == baja_2_index) & (labels != baja_2_index))\n",
    "\n",
    "    # Calculate ganancia\n",
    "    ganancia = (reward_baja_2 * correct_baja_2) + (penalty_baja_2 * incorrect_baja_2)\n",
    "\n",
    "    return ganancia\n",
    "\n",
    "\n",
    "def compute_rewards_ordinal(preds, labels, mask=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compute rewards based on ordinal classification predictions.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Predicted class indices from the actor.\n",
    "                               Shape: [batch_size, seq_len]\n",
    "        labels (torch.Tensor): Ground truth class indices.\n",
    "                               Shape: [batch_size, seq_len]\n",
    "        mask (torch.Tensor, optional): Mask tensor where 1 indicates valid timesteps.\n",
    "                                       Shape: [batch_size, seq_len]\n",
    "        device (str): Device where tensors are located.\n",
    "\n",
    "    Returns:\n",
    "        rewards (torch.Tensor): Computed rewards.\n",
    "                                Shape: [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    #print(\"  [compute_rewards_ordinal] Starting computation...\")\n",
    "    # Define target labels and corresponding rewards\n",
    "    target_labels = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 69.0], device=device)\n",
    "    label_rewards = torch.tensor([3.0, 117.0, 3.0, 3.0, 1.0, 69.0], device=device)\n",
    "\n",
    "    # Ensure preds and labels are of shape [batch_size, seq_len]\n",
    "    preds = preds.long()  # Ensure integer type\n",
    "    labels = labels.long()\n",
    "\n",
    "    #print(f\"  [compute_rewards_ordinal] preds shape: {preds.shape}\")\n",
    "    #print(f\"  [compute_rewards_ordinal] labels shape: {labels.shape}\")\n",
    "\n",
    "    # Initialize rewards tensor\n",
    "    rewards = torch.zeros_like(preds, dtype=torch.float, device=device)  # [batch_size, seq_len]\n",
    "    #print(f\"  [compute_rewards_ordinal] Initialized rewards shape: {rewards.shape}\")\n",
    "\n",
    "    # ===== Correct Predictions =====\n",
    "    correct_predictions = (preds == labels)  # [batch_size, seq_len]\n",
    "    #print(f\"  [compute_rewards_ordinal] correct_predictions shape: {correct_predictions.shape}\")\n",
    "\n",
    "    rewards += correct_predictions.float() * label_rewards[labels]  # Assign rewards for correct predictions\n",
    "    #print(f\"  [compute_rewards_ordinal] Rewards after correct predictions: {rewards}\")\n",
    "\n",
    "    # ===== Incorrect Predictions =====\n",
    "    # Define penalty matrix\n",
    "    penalty_matrix = torch.zeros(6, 6, device=device)  # [6,6]\n",
    "\n",
    "    # Populate the penalty matrix based on misclassification rules\n",
    "    # Index mapping:\n",
    "    # 0: 1.0 ('BAJA+1')\n",
    "    # 1: 2.0 ('BAJA+2')\n",
    "    # 2: 3.0 ('BAJA+3')\n",
    "    # 3: 4.0 ('BAJA+4')\n",
    "    # 4: 5.0 ('CONTINUA')\n",
    "    # 5: 69.0 ('OUT')\n",
    "\n",
    "    # Penalties as per original function\n",
    "    penalty_matrix[1, 0] = -3.0      # Predicted 'BAJA+2' when true label is 'BAJA+1'\n",
    "    penalty_matrix[1, 2] = -3.0      # Predicted 'BAJA+2' when true label is 'BAJA+3'    \n",
    "    penalty_matrix[1, 3] = -3.0      # Predicted 'BAJA+2' when true label is 'BAJA+4'\n",
    "    penalty_matrix[1, 4] = -3.0      # Predicted 'BAJA+2' when true label is 'CONTINUA'\n",
    "\n",
    "    penalty_matrix[0, 1] = -117.0    # Predicted 'BAJA+1' when true label is 'BAJA+2'\n",
    "    penalty_matrix[0, 2] = -3.0      # Predicted 'BAJA+1' when true label is 'BAJA+3'   \n",
    "    penalty_matrix[0, 3] = -3.0      # Predicted 'BAJA+1' when true label is 'BAJA+4'\n",
    "    penalty_matrix[0, 4] = -3.0      # Predicted 'BAJA+1' when true label is 'CONTINUA'\n",
    "\n",
    "    penalty_matrix[2, 0] = -3.0      # Predicted 'BAJA+3' when true label is 'BAJA+1'\n",
    "    penalty_matrix[2, 1] = -117.0    # Predicted 'BAJA+3' when true label is 'BAJA+2'  \n",
    "    penalty_matrix[2, 3] = -3.0      # Predicted 'BAJA+3' when true label is 'BAJA+4'\n",
    "    penalty_matrix[2, 4] = -3.0      # Predicted 'BAJA+3' when true label is 'CONTINUA'\n",
    "\n",
    "    penalty_matrix[3, 0] = -3.0      # Predicted 'BAJA+4' when true label is 'BAJA+1'\n",
    "    penalty_matrix[3, 1] = -117.0    # Predicted 'BAJA+4' when true label is 'BAJA+2'  \n",
    "    penalty_matrix[3, 2] = -3.0      # Predicted 'BAJA+4' when true label is 'BAJA+3'\n",
    "    penalty_matrix[3, 4] = -3.0      # Predicted 'BAJA+4' when true label is 'CONTINUA'\n",
    "\n",
    "    penalty_matrix[4, 0] = -3.0      # Predicted 'CONTINUA' when true label is 'BAJA+1'\n",
    "    penalty_matrix[4, 1] = -117.0    # Predicted 'CONTINUA' when true label is 'BAJA+2'  \n",
    "    penalty_matrix[4, 2] = -3.0      # Predicted 'CONTINUA' when true label is 'BAJA+3'\n",
    "    penalty_matrix[4, 3] = -3.0      # Predicted 'CONTINUA' when true label is 'BAJA+4'\n",
    "\n",
    "    #print(f\"  [compute_rewards_ordinal] penalty_matrix:\\n{penalty_matrix}\")\n",
    "\n",
    "    # Identify incorrect predictions\n",
    "    incorrect_predictions = ~correct_predictions  # [batch_size, seq_len]\n",
    "    #print(f\"  [compute_rewards_ordinal] incorrect_predictions shape: {incorrect_predictions.shape}\")\n",
    "\n",
    "    predicted_indices = preds[incorrect_predictions]        # [num_incorrect]\n",
    "    true_indices = labels[incorrect_predictions]            # [num_incorrect]\n",
    "    #print(f\"  [compute_rewards_ordinal] predicted_indices shape: {predicted_indices.shape}\")\n",
    "    #print(f\"  [compute_rewards_ordinal] true_indices shape: {true_indices.shape}\")\n",
    "\n",
    "    # Gather penalties from the penalty matrix\n",
    "    penalties = penalty_matrix[predicted_indices, true_indices]  # [num_incorrect]\n",
    "    #print(f\"  [compute_rewards_ordinal] penalties shape: {penalties.shape}\")\n",
    "    #print(f\"  [compute_rewards_ordinal] penalties: {penalties}\")\n",
    "\n",
    "    # Assign penalties to the rewards tensor\n",
    "    rewards[incorrect_predictions] += penalties\n",
    "    #print(f\"  [compute_rewards_ordinal] Rewards after penalties: {rewards}\")\n",
    "\n",
    "    # ===== Scaling Rewards =====\n",
    "    rewards = rewards * 0.25  # Scale rewards as per original function\n",
    "    #print(f\"  [compute_rewards_ordinal] Rewards after scaling: {rewards}\")\n",
    "\n",
    "    # ===== Normalize Rewards =====\n",
    "    #mean_reward = rewards.mean()\n",
    "    #std_reward = rewards.std() + 1e-8  # Prevent division by zero\n",
    "    #rewards = (rewards - mean_reward) / std_reward\n",
    "\n",
    "    # ===== Apply Mask =====\n",
    "    if mask is not None:\n",
    "        rewards = rewards * mask.float()  # Apply mask to rewards\n",
    "        #print(f\"  [compute_rewards_ordinal] Rewards after applying mask: {rewards}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc7a88a5-cdde-475e-8545-4b3ed1a01a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardWeightedBCEOrdinalLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(RewardWeightedBCEOrdinalLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')  # Compute BCE without reduction\n",
    "\n",
    "    def forward(self, \n",
    "                action_probs,      # [batch_size, seq_len, K-1]\n",
    "                target_labels,     # [batch_size, seq_len]\n",
    "                rewards,           # [batch_size, seq_len]\n",
    "                mask=None          # [batch_size, seq_len]\n",
    "               ):\n",
    "        batch_size, seq_len, K_minus_1 = action_probs.shape\n",
    "\n",
    "        # Convert target_labels to binary threshold labels\n",
    "        target_thresholds = torch.zeros_like(action_probs)  # [batch_size, seq_len, K-1]\n",
    "        for i in range(K_minus_1):\n",
    "            target_thresholds[:, :, i] = (target_labels > i).float()\n",
    "\n",
    "        # Compute BCE loss for each threshold\n",
    "        bce = self.bce_loss(action_probs, target_thresholds)  # [batch_size, seq_len, K-1]\n",
    "       \n",
    "        \n",
    "        # Create masks for correct and incorrect predictions\n",
    "        correct_mask = target_thresholds.bool()   # [batch_size, seq_len, K-1]\n",
    "        incorrect_mask = ~correct_mask            # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Expand rewards to match the thresholds\n",
    "        rewards_expanded = rewards.unsqueeze(-1)   # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Ensure rewards_expanded has the correct shape\n",
    "        assert rewards_expanded.dim() == 3 and rewards_expanded.size(-1) == 1, \\\n",
    "            f\"Expected rewards_expanded to have shape [batch_size, seq_len, 1], but got {rewards_expanded.shape}\"\n",
    "\n",
    "        reward_positive = rewards_expanded * correct_mask.float()    # [batch_size, seq_len, K-1]\n",
    "        reward_negative = (-rewards_expanded) * incorrect_mask.float()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Define a small positive value to avoid zero weighting\n",
    "        min_weight = 1e-3\n",
    "        reward_positive = torch.clamp(reward_positive, min=min_weight)\n",
    "        reward_negative = torch.clamp(reward_negative, min=min_weight)\n",
    "\n",
    "        # Combine the weights: positive rewards for correct, penalties for incorrect\n",
    "        weights = reward_positive + reward_negative  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Ensure weights are non-negative\n",
    "        weights = torch.clamp(weights, min=0.0)\n",
    "\n",
    "\n",
    "\n",
    "        # Weight the loss by rewards\n",
    "        weighted_bce = bce * weights  # [batch_size, seq_len]\n",
    "\n",
    "        # Diagnostic Print Statements\n",
    "        unique_thresholds = torch.unique(target_thresholds)\n",
    "        #print(f\"[Loss Computation] Unique target_thresholds: {unique_thresholds}\")\n",
    "        assert torch.all((unique_thresholds == 0) | (unique_thresholds == 1)), \"Target thresholds contain values other than 0 and 1.\"\n",
    "\n",
    "        #print(f\"[Loss Computation] Rewards stats: min={rewards.min().item()}, max={rewards.max().item()}, mean={rewards.mean().item()}\")\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            #print(f\"[Loss Computation] Mask sum: {mask.sum().item()}, Mask shape: {mask.shape}\")\n",
    "            weighted_bce = weighted_bce * mask.float()\n",
    "            loss = weighted_bce.sum() / (mask.sum() + self.epsilon)\n",
    "        else:\n",
    "            loss = weighted_bce.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class RewardWeightedBCEOrdinalLossasd(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(RewardWeightedBCEOrdinalLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')  # Compute BCE without reduction\n",
    "\n",
    "    def forward(self, \n",
    "                action_probs,      # [batch_size, seq_len, K-1]\n",
    "                target_labels,     # [batch_size, seq_len]\n",
    "                rewards,           # [batch_size, seq_len]\n",
    "                mask=None          # [batch_size, seq_len]\n",
    "               ):\n",
    "        batch_size, seq_len, K_minus_1 = action_probs.shape\n",
    "\n",
    "        # Convert target_labels to binary threshold labels\n",
    "        target_thresholds = torch.zeros_like(action_probs)  # [batch_size, seq_len, K-1]\n",
    "        for i in range(K_minus_1):\n",
    "            target_thresholds[:, :, i] = (target_labels > i).float()\n",
    "\n",
    "        # Compute BCE loss for each threshold\n",
    "        bce = self.bce_loss(action_probs, target_thresholds)  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Create masks for correct and incorrect predictions\n",
    "        correct_mask = target_thresholds.bool()   # [batch_size, seq_len, K-1]\n",
    "        incorrect_mask = ~correct_mask            # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Expand rewards to match the thresholds\n",
    "        rewards_expanded = rewards.unsqueeze(-1)   # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Ensure rewards_expanded has the correct shape\n",
    "        assert rewards_expanded.dim() == 3 and rewards_expanded.size(-1) == 1, \\\n",
    "            f\"Expected rewards_expanded to have shape [batch_size, seq_len, 1], but got {rewards_expanded.shape}\"\n",
    "\n",
    "        reward_positive = rewards_expanded * correct_mask.float()    # [batch_size, seq_len, K-1]\n",
    "        reward_negative = (-rewards_expanded) * incorrect_mask.float()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Define a small positive value to avoid zero weighting\n",
    "        min_weight = 1e-3\n",
    "        reward_positive = torch.clamp(reward_positive, min=min_weight)\n",
    "        reward_negative = torch.clamp(reward_negative, min=min_weight)\n",
    "\n",
    "        # Combine the weights: positive rewards for correct, penalties for incorrect\n",
    "        weights = reward_positive + reward_negative  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Ensure weights are non-negative\n",
    "        weights = torch.clamp(weights, min=0.0)\n",
    "\n",
    "        # Weight the loss by rewards\n",
    "        weighted_bce = bce * weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Unsqueeze mask to match the dimensions\n",
    "            mask_expanded = mask.float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            weighted_bce = weighted_bce * mask_expanded  # Broadcasting applies here\n",
    "            loss = weighted_bce.sum() / (mask_expanded.sum() + self.epsilon)\n",
    "        else:\n",
    "            loss = weighted_bce.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class RewardWeightedBCEOrdinalLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(RewardWeightedBCEOrdinalLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')  # Compute BCE without reduction\n",
    "\n",
    "    def forward(self, \n",
    "                action_probs,      # [batch_size, seq_len, K-1]\n",
    "                target_labels,     # [batch_size, seq_len]\n",
    "                rewards,           # [batch_size, seq_len]\n",
    "                mask=None,         # [batch_size, seq_len]\n",
    "                class_weights=None # [num_classes]\n",
    "               ):\n",
    "        batch_size, seq_len, K_minus_1 = action_probs.shape\n",
    "\n",
    "        # Convert target_labels to binary threshold labels\n",
    "        target_thresholds = torch.zeros_like(action_probs)  # [batch_size, seq_len, K-1]\n",
    "        for i in range(K_minus_1):\n",
    "            target_thresholds[:, :, i] = (target_labels > i).float()\n",
    "\n",
    "        #print(\"action_probs min:\", action_probs.min().item(), \"max:\", action_probs.max().item())\n",
    "        #print(\"Any NaNs in action_probs:\", torch.isnan(action_probs).any().item())\n",
    "\n",
    "\n",
    "        # Compute BCE loss for each threshold\n",
    "        bce = self.bce_loss(action_probs, target_thresholds)  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Create masks for correct and incorrect predictions\n",
    "        correct_mask = target_thresholds.bool()   # [batch_size, seq_len, K-1]\n",
    "        incorrect_mask = ~correct_mask            # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Expand rewards to match the thresholds\n",
    "        rewards_expanded = rewards.unsqueeze(-1)   # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Ensure rewards_expanded has the correct shape\n",
    "        assert rewards_expanded.dim() == 3 and rewards_expanded.size(-1) == 1, \\\n",
    "            f\"Expected rewards_expanded to have shape [batch_size, seq_len, 1], but got {rewards_expanded.shape}\"\n",
    "\n",
    "        reward_positive = rewards_expanded * correct_mask.float()    # [batch_size, seq_len, K-1]\n",
    "        reward_negative = (-rewards_expanded) * incorrect_mask.float()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Define a small positive value to avoid zero weighting\n",
    "        min_weight = 1e-3\n",
    "        reward_positive = torch.clamp(reward_positive, min=min_weight)\n",
    "        reward_negative = torch.clamp(reward_negative, min=min_weight)\n",
    "\n",
    "        # Combine the weights: positive rewards for correct, penalties for incorrect\n",
    "        weights = reward_positive + reward_negative  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Ensure weights are non-negative\n",
    "        weights = torch.clamp(weights, min=0.0)\n",
    "\n",
    "        # Apply class weights if provided\n",
    "        if class_weights is not None:\n",
    "            #print(\"usin cw\")\n",
    "            # Get the class weight for each sample based on its true label\n",
    "            sample_weights = class_weights[target_labels]  # [batch_size, seq_len]\n",
    "            sample_weights = sample_weights.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            # Multiply the existing weights by the class weights\n",
    "            weights = weights * sample_weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Weight the loss by rewards and class weights\n",
    "        weighted_bce = bce * weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Unsqueeze mask to match the dimensions\n",
    "            mask_expanded = mask.float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            weighted_bce = weighted_bce * mask_expanded  # Broadcasting applies here\n",
    "            loss = weighted_bce.sum() / (mask_expanded.sum() + self.epsilon)\n",
    "        else:\n",
    "            loss = weighted_bce.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Reward Weighted BCE Ordinal Loss\n",
    "# -------------------------------\n",
    "class RewardWeightedBCEOrdinalLossasd(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(RewardWeightedBCEOrdinalLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')  # Compute BCE without reduction\n",
    "\n",
    "    def forward(self, \n",
    "                action_probs,      # [batch_size, seq_len, K-1]\n",
    "                target_labels,     # [batch_size, seq_len]\n",
    "                rewards,           # [batch_size, seq_len]\n",
    "                mask=None,         # [batch_size, seq_len]\n",
    "                class_weights=None # [num_classes]\n",
    "               ):\n",
    "        batch_size, seq_len, K_minus_1 = action_probs.shape\n",
    "\n",
    "        # Convert target_labels to binary threshold labels\n",
    "        target_thresholds = torch.zeros_like(action_probs)  # [batch_size, seq_len, K-1]\n",
    "        for i in range(K_minus_1):\n",
    "            target_thresholds[:, :, i] = (target_labels > i).float()\n",
    "\n",
    "        # Compute BCE loss for each threshold\n",
    "        bce = self.bce_loss(action_probs, target_thresholds)  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Create masks for correct and incorrect predictions\n",
    "        correct_mask = target_thresholds.bool()   # [batch_size, seq_len, K-1]\n",
    "        incorrect_mask = ~correct_mask            # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Expand rewards to match the thresholds\n",
    "        rewards_expanded = rewards.unsqueeze(-1)   # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Ensure rewards_expanded has the correct shape\n",
    "        assert rewards_expanded.dim() == 3 and rewards_expanded.size(-1) == 1, \\\n",
    "            f\"Expected rewards_expanded to have shape [batch_size, seq_len, 1], but got {rewards_expanded.shape}\"\n",
    "\n",
    "        reward_positive = rewards_expanded * correct_mask.float()    # [batch_size, seq_len, K-1]\n",
    "        reward_negative = (-rewards_expanded) * incorrect_mask.float()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Define a small positive value to avoid zero weighting\n",
    "        min_weight = 1e-3\n",
    "        reward_positive = torch.clamp(reward_positive, min=min_weight)\n",
    "        reward_negative = torch.clamp(reward_negative, min=min_weight)\n",
    "\n",
    "        # Combine the weights: positive rewards for correct, penalties for incorrect\n",
    "        weights = reward_positive + reward_negative  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Ensure weights are non-negative\n",
    "        weights = torch.clamp(weights, min=0.0)\n",
    "\n",
    "        # Apply class weights if provided\n",
    "        if class_weights is not None:\n",
    "            # Assuming class_weights is a tensor of shape [num_classes]\n",
    "            # and target_labels contain class indices [0, num_classes-1]\n",
    "            # Here, adjust if class_weights should be applied per threshold\n",
    "            sample_weights = class_weights[target_labels]  # [batch_size, seq_len]\n",
    "            sample_weights = sample_weights.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            # Multiply the existing weights by the class weights\n",
    "            weights = weights * sample_weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Weight the loss by rewards and class weights\n",
    "        weighted_bce = bce * weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Unsqueeze mask to match the dimensions\n",
    "            mask_expanded = mask.float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            weighted_bce = weighted_bce * mask_expanded  # Broadcasting applies here\n",
    "            loss = weighted_bce.sum() / (mask_expanded.sum() + self.epsilon)\n",
    "        else:\n",
    "            loss = weighted_bce.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RewardWeightedBCEOrdinalLossasd(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5):\n",
    "        super(RewardWeightedBCEOrdinalLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.bce_loss = nn.BCELoss(reduction='none')  # Compute BCE without reduction\n",
    "\n",
    "    def forward(self, \n",
    "                action_probs,      # [batch_size, seq_len, K-1]\n",
    "                target_labels,     # [batch_size, seq_len]\n",
    "                rewards,           # [batch_size, seq_len]\n",
    "                mask=None,         # [batch_size, seq_len]\n",
    "                class_weights=None # [num_classes]\n",
    "               ):\n",
    "        batch_size, seq_len, K_minus_1 = action_probs.shape\n",
    "\n",
    "        # Apply causal mask to action_probs to prevent NaNs\n",
    "        device = action_probs.device\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()  # [seq_len, seq_len]\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(-1)  # [1, seq_len, seq_len, 1]\n",
    "\n",
    "        # Expand action_probs to [batch_size, seq_len, seq_len, K-1] for masking\n",
    "        action_probs_expanded = action_probs.unsqueeze(1).expand(-1, seq_len, -1, -1)\n",
    "        action_probs_expanded = action_probs_expanded.masked_fill(causal_mask, 0.0)\n",
    "\n",
    "        # Now, action_probs_expanded is [batch_size, seq_len, seq_len, K-1]\n",
    "        # We take the diagonal elements to get the original action_probs but with zeros where masked\n",
    "        action_probs = torch.diagonal(action_probs_expanded, dim1=1, dim2=2).permute(0, 2, 1)  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Debugging prints\n",
    "        #print(f\"action_probs shape: {action_probs.shape}\")\n",
    "        #print(f\"target_labels shape: {target_labels.shape}\")\n",
    "        #print(f\"rewards shape: {rewards.shape}\")\n",
    "        #print(f\"action_probs min: {action_probs.min().item()}, max: {action_probs.max().item()}\")\n",
    "\n",
    "        # Convert target_labels to binary threshold labels\n",
    "        target_thresholds = torch.zeros_like(action_probs)  # [batch_size, seq_len, K-1]\n",
    "        for i in range(K_minus_1):\n",
    "            target_thresholds[:, :, i] = (target_labels > i).float()\n",
    "\n",
    "        # Compute BCE loss for each threshold\n",
    "        bce = self.bce_loss(action_probs, target_thresholds)  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Create masks for correct and incorrect predictions\n",
    "        correct_mask = target_thresholds.bool()   # [batch_size, seq_len, K-1]\n",
    "        incorrect_mask = ~correct_mask            # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Expand rewards to match the thresholds\n",
    "        rewards_expanded = rewards.unsqueeze(-1)   # [batch_size, seq_len, 1]\n",
    "\n",
    "        # Ensure rewards_expanded has the correct shape\n",
    "        assert rewards_expanded.dim() == 3 and rewards_expanded.size(-1) == 1, \\\n",
    "            f\"Expected rewards_expanded to have shape [batch_size, seq_len, 1], but got {rewards_expanded.shape}\"\n",
    "\n",
    "        reward_positive = rewards_expanded * correct_mask.float()    # [batch_size, seq_len, K-1]\n",
    "        reward_negative = (-rewards_expanded) * incorrect_mask.float()  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Define a small positive value to avoid zero weighting\n",
    "        min_weight = 1e-3\n",
    "        reward_positive = torch.clamp(reward_positive, min=min_weight)\n",
    "        reward_negative = torch.clamp(reward_negative, min=min_weight)\n",
    "\n",
    "        # Combine the weights: positive rewards for correct, penalties for incorrect\n",
    "        weights = reward_positive + reward_negative  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Ensure weights are non-negative\n",
    "        weights = torch.clamp(weights, min=0.0)\n",
    "\n",
    "        # Apply class weights if provided\n",
    "        if class_weights is not None:\n",
    "            # Assuming class_weights is a tensor of shape [num_classes]\n",
    "            # and target_labels contain class indices [0, num_classes-1]\n",
    "            # Here, adjust if class_weights should be applied per threshold\n",
    "            sample_weights = class_weights[target_labels]  # [batch_size, seq_len]\n",
    "            sample_weights = sample_weights.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            # Multiply the existing weights by the class weights\n",
    "            weights = weights * sample_weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Weight the loss by rewards and class weights\n",
    "        weighted_bce = bce * weights  # [batch_size, seq_len, K-1]\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Unsqueeze mask to match the dimensions\n",
    "            mask_expanded = mask.float().unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "            weighted_bce = weighted_bce * mask_expanded  # Broadcasting applies here\n",
    "            loss = weighted_bce.sum() / (mask_expanded.sum() + self.epsilon)\n",
    "        else:\n",
    "            loss = weighted_bce.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc25f0d7-78ed-4050-9329-079b63c77e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Existing Classes: RotaryPositionalEmbeddingROPE, MultiheadAttentionWithRoPE, TransformerEncoderLayerWithRoPE\n",
    "\n",
    "class RotaryPositionalEmbeddingROPE(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super(RotaryPositionalEmbeddingROPE, self).__init__()\n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError(f\"The dimension for RoPE must be even. {dim}\")\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_heads, seq_len, head_dim = x.size()\n",
    "        device = x.device\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(0).unsqueeze(2)\n",
    "        dim_t = torch.arange(0, head_dim, 2, dtype=torch.float, device=device)\n",
    "        dim_t = self.base ** (-dim_t / head_dim)\n",
    "        angles = position_ids * dim_t\n",
    "        sin_angles = angles.sin().unsqueeze(0).unsqueeze(2)\n",
    "        cos_angles = angles.cos().unsqueeze(0).unsqueeze(2)\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        x_rotated = torch.cat([x1 * cos_angles - x2 * sin_angles,\n",
    "                               x1 * sin_angles + x2 * cos_angles], dim=-1)\n",
    "        return x_rotated\n",
    "\n",
    "class MultiheadAttentionWithRoPE(nn.MultiheadAttention):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, bias=True, add_bias_kv=False,\n",
    "                 add_zero_attn=False, kdim=None, vdim=None, base=10000, batch_first=True):\n",
    "        super(MultiheadAttentionWithRoPE, self).__init__(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=add_zero_attn,\n",
    "            kdim=kdim,\n",
    "            vdim=vdim,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbeddingROPE(self.head_dim, base=base)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=True,\n",
    "                average_attn_weights=True, is_causal=True, **kwargs):\n",
    "        if self.batch_first:\n",
    "            Q = query.view(query.size(0), query.size(1), self.num_heads, self.head_dim).transpose(1,2)\n",
    "            K = key.view(key.size(0), key.size(1), self.num_heads, self.head_dim).transpose(1,2)\n",
    "            Q = self.rotary_emb(Q)\n",
    "            K = self.rotary_emb(K)\n",
    "            Q = Q.transpose(1,2).contiguous().view(query.size(0), query.size(1), self.embed_dim)\n",
    "            K = K.transpose(1,2).contiguous().view(key.size(0), key.size(1), self.embed_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\"batch_first=False is not supported in this custom attention.\")\n",
    "        attn_output, attn_weights = super(MultiheadAttentionWithRoPE, self).forward(\n",
    "            Q, K, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask,\n",
    "            need_weights=need_weights, average_attn_weights=average_attn_weights\n",
    "        )\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class TransformerEncoderLayerWithRoPE(TransformerEncoderLayer):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1, base=10000, activation='relu', batch_first=True):\n",
    "        super(TransformerEncoderLayerWithRoPE, self).__init__(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.self_attn = MultiheadAttentionWithRoPE(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            base=base,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "class ActorCriticOrdinalTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=512, num_heads=8, num_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.2, num_actor_heads=10, \n",
    "                 num_classes=5, base=10000):\n",
    "        super(ActorCriticOrdinalTransformerModel, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_thresholds = num_classes - 1\n",
    "        \n",
    "        # Input projection\n",
    "        self.linear_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Actor Transformer Encoder with RoPE\n",
    "        actor_encoder_layer = TransformerEncoderLayerWithRoPE(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            base=base,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.actor_transformer_encoder = TransformerEncoder(actor_encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Actor Heads\n",
    "        self.actor_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, dim_feedforward),\n",
    "                nn.LeakyReLU(negative_slope=0.01),\n",
    "                nn.LayerNorm(dim_feedforward),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(dim_feedforward, d_model),\n",
    "                nn.LeakyReLU(negative_slope=0.01),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Linear(d_model, self.num_thresholds)  # Output logits for thresholds\n",
    "            )\n",
    "            for _ in range(num_actor_heads)\n",
    "        ])\n",
    "        \n",
    "        # Learnable Raw Thresholds (before ordering)\n",
    "        # Initialize raw thresholds; values will be transformed to ensure ordering\n",
    "        self.raw_thresholds = nn.Parameter(torch.linspace(-1, 1, steps=self.num_thresholds))\n",
    "    \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size, seq_len, input_dim]\n",
    "            src_key_padding_mask: [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            action_probs: [batch_size, seq_len, K-1] (Cumulative Probabilities)\n",
    "        \"\"\"\n",
    "        # Project input to model dimension\n",
    "        src = self.linear_proj(src)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Actor Transformer Encoding\n",
    "        actor_transformer_output = self.actor_transformer_encoder(\n",
    "            src, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Actor Heads Processing\n",
    "        actor_outputs = [actor_head(actor_transformer_output) for actor_head in self.actor_heads]\n",
    "        actor_outputs = torch.stack(actor_outputs, dim=0)  # [num_actor_heads, batch_size, seq_len, K-1]\n",
    "        action_preds = actor_outputs.mean(dim=0)  # [batch_size, seq_len, K-1]\n",
    "        \n",
    "        # Ordering and Applying Thresholds\n",
    "        # Apply softplus to raw thresholds to ensure positivity, then cumulative sum to ensure ordering\n",
    "        thresholds = torch.cumsum(F.softplus(self.raw_thresholds), dim=0)  # [K-1]\n",
    "        thresholds = thresholds.unsqueeze(0).unsqueeze(0)  # [1, 1, K-1]\n",
    "        \n",
    "        # Compute Cumulative Probabilities using CLM\n",
    "        # P(Y <= k | X) = sigmoid(logit(Y <= k | X) - theta_k)\n",
    "        cumulative_probs = torch.sigmoid(action_preds - thresholds)  # [batch_size, seq_len, K-1]\n",
    "        \n",
    "        # Assertions to ensure correct output shapes\n",
    "        assert cumulative_probs.shape[-1] == self.num_thresholds, \\\n",
    "            f\"Expected cumulative_probs to have last dimension {self.num_thresholds}, but got {cumulative_probs.shape[-1]}\"\n",
    "        \n",
    "        return cumulative_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------TCN--------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class MultiheadAttentionWithRoPE(nn.MultiheadAttention):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, bias=True, add_bias_kv=False,\n",
    "                 add_zero_attn=False, kdim=None, vdim=None, base=10000, batch_first=True):\n",
    "        super(MultiheadAttentionWithRoPE, self).__init__(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=add_zero_attn,\n",
    "            kdim=kdim,\n",
    "            vdim=vdim,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbeddingROPE(self.head_dim, base=base)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=True,\n",
    "                average_attn_weights=True, is_causal=True, **kwargs):\n",
    "        if self.batch_first:\n",
    "            Q = query.view(query.size(0), query.size(1), self.num_heads, self.head_dim).transpose(1,2)\n",
    "            K = key.view(key.size(0), key.size(1), self.num_heads, self.head_dim).transpose(1,2)\n",
    "            Q = self.rotary_emb(Q)\n",
    "            K = self.rotary_emb(K)\n",
    "            Q = Q.transpose(1,2).contiguous().view(query.size(0), query.size(1), self.embed_dim)\n",
    "            K = K.transpose(1,2).contiguous().view(key.size(0), key.size(1), self.embed_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\"batch_first=False is not supported in this custom attention.\")\n",
    "        attn_output, attn_weights = super(MultiheadAttentionWithRoPE, self).forward(\n",
    "            Q, K, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask,\n",
    "            need_weights=need_weights, average_attn_weights=average_attn_weights\n",
    "        )\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class RotaryPositionalEmbeddingROPE(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super(RotaryPositionalEmbeddingROPE, self).__init__()\n",
    "        if dim % 2 != 0:\n",
    "            raise ValueError(f\"The dimension for RoPE must be even. {dim}\")\n",
    "        self.dim = dim\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_heads, seq_len, head_dim = x.size()\n",
    "        device = x.device\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(0).unsqueeze(2)\n",
    "        dim_t = torch.arange(0, head_dim, 2, dtype=torch.float, device=device)\n",
    "        dim_t = self.base ** (-dim_t / head_dim)\n",
    "        angles = position_ids * dim_t\n",
    "        sin_angles = angles.sin().unsqueeze(0).unsqueeze(2)\n",
    "        cos_angles = angles.cos().unsqueeze(0).unsqueeze(2)\n",
    "        x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "        x_rotated = torch.cat([x1 * cos_angles - x2 * sin_angles,\n",
    "                               x1 * sin_angles + x2 * cos_angles], dim=-1)\n",
    "        return x_rotated\n",
    "\n",
    "class TransformerEncoderLayerWithRoPE(TransformerEncoderLayer):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1, base=10000, activation='relu', batch_first=True):\n",
    "        super(TransformerEncoderLayerWithRoPE, self).__init__(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.self_attn = MultiheadAttentionWithRoPE(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            base=base,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, dilation=dilation)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Downsample if in_channels != out_channels\n",
    "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, in_channels, seq_len]\n",
    "            mask: [batch_size, seq_len] (1 for valid tokens, 0 for padding)\n",
    "        \n",
    "        Returns:\n",
    "            out: [batch_size, out_channels, seq_len]\n",
    "        \"\"\"\n",
    "        # Causal padding: pad only on the left\n",
    "        padding = (self.kernel_size - 1) * self.dilation\n",
    "        x_padded = F.pad(x, (padding, 0))  # Pad left side\n",
    "        \n",
    "        out = self.conv1(x_padded)  # [batch_size, out_channels, seq_len]\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Expand mask to [batch_size, 1, seq_len] for broadcasting\n",
    "            mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len]\n",
    "            out = out * mask  # Zero out padded positions\n",
    "        \n",
    "        # Causal padding again for second convolution\n",
    "        out_padded = F.pad(out, (padding, 0))  # Pad left side again\n",
    "        out = self.conv2(out_padded)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        if mask is not None:\n",
    "            out = out * mask  # Zero out padded positions\n",
    "        \n",
    "        # Residual connection\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        if mask is not None:\n",
    "            res = res * mask  # Zero out residuals for padded positions\n",
    "        out += res  # [batch_size, out_channels, seq_len]\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ActorCriticOrdinalTransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim, \n",
    "        d_model=512, \n",
    "        num_heads=8, \n",
    "        num_layers=6, \n",
    "        dim_feedforward=2048, \n",
    "        dropout=0.2, \n",
    "        num_actor_heads=10, \n",
    "        num_classes=5, \n",
    "        base=10000,\n",
    "        tcn_kernel_size=3,\n",
    "        tcn_dilation=1,\n",
    "        tcn_dropout=0.2\n",
    "    ):\n",
    "        super(ActorCriticOrdinalTransformerModel, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_thresholds = num_classes - 1\n",
    "        \n",
    "        # Input projection\n",
    "        self.linear_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Actor Transformer Encoder with RoPE\n",
    "        actor_encoder_layer = TransformerEncoderLayerWithRoPE(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            base=base,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.actor_transformer_encoder = TransformerEncoder(actor_encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # TCN Block\n",
    "        self.tcn = TCNBlock(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=tcn_kernel_size,\n",
    "            dilation=tcn_dilation,\n",
    "            dropout=tcn_dropout\n",
    "        )\n",
    "        \n",
    "        # Actor Heads\n",
    "        self.actor_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, dim_feedforward),\n",
    "                nn.LeakyReLU(negative_slope=0.01),\n",
    "                nn.LayerNorm(dim_feedforward),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(dim_feedforward, d_model),\n",
    "                nn.LeakyReLU(negative_slope=0.01),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Linear(d_model, self.num_thresholds)  # Output logits for thresholds\n",
    "            )\n",
    "            for _ in range(num_actor_heads)\n",
    "        ])\n",
    "        \n",
    "        # Learnable Raw Thresholds (before ordering)\n",
    "        # Initialize raw thresholds; values will be transformed to ensure ordering\n",
    "        self.raw_thresholds = nn.Parameter(torch.linspace(-1, 1, steps=self.num_thresholds))\n",
    "    \n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: [batch_size, seq_len, input_dim]\n",
    "            src_key_padding_mask: [batch_size, seq_len] (True for padding, False otherwise)\n",
    "        \n",
    "        Returns:\n",
    "            action_probs: [batch_size, seq_len, K-1] (Cumulative Probabilities)\n",
    "        \"\"\"\n",
    "        # Project input to model dimension\n",
    "        src = self.linear_proj(src)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Actor Transformer Encoding\n",
    "        actor_transformer_output = self.actor_transformer_encoder(\n",
    "            src, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Prepare for TCN: Transpose to [batch_size, d_model, seq_len]\n",
    "        tcn_input = actor_transformer_output.permute(0, 2, 1)  # [batch_size, d_model, seq_len]\n",
    "        \n",
    "        # TCN Processing with Mask\n",
    "        if src_key_padding_mask is not None:\n",
    "            # Convert mask: True for padding, False for valid tokens\n",
    "            # We need 1 for valid tokens and 0 for padding\n",
    "            tcn_mask = ~src_key_padding_mask  # Invert mask\n",
    "            tcn_mask = tcn_mask.float()  # [batch_size, seq_len]\n",
    "        else:\n",
    "            tcn_mask = None\n",
    "        \n",
    "        tcn_output = self.tcn(tcn_input, mask=tcn_mask)  # [batch_size, d_model, seq_len]\n",
    "        \n",
    "        # Transpose back to [batch_size, seq_len, d_model]\n",
    "        tcn_output = tcn_output.permute(0, 2, 1)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Actor Heads Processing\n",
    "        actor_outputs = [actor_head(tcn_output) for actor_head in self.actor_heads]\n",
    "        actor_outputs = torch.stack(actor_outputs, dim=0)  # [num_actor_heads, batch_size, seq_len, K-1]\n",
    "        action_preds = actor_outputs.mean(dim=0)  # [batch_size, seq_len, K-1]\n",
    "        \n",
    "        # Ordering and Applying Thresholds\n",
    "        thresholds = torch.cumsum(F.softplus(self.raw_thresholds), dim=0).unsqueeze(0).unsqueeze(0)  # [1, 1, K-1]\n",
    "        cumulative_probs = torch.sigmoid(action_preds - thresholds)  # [batch_size, seq_len, K-1]\n",
    "        \n",
    "        # Zero out probabilities for padded positions\n",
    "        if src_key_padding_mask is not None:\n",
    "            cumulative_probs = cumulative_probs.masked_fill(src_key_padding_mask.unsqueeze(-1), 0.0)\n",
    "        \n",
    "        # Assertions to ensure correct output shapes\n",
    "        assert cumulative_probs.shape[-1] == self.num_thresholds, \\\n",
    "            f\"Expected cumulative_probs to have last dimension {self.num_thresholds}, but got {cumulative_probs.shape[-1]}\"\n",
    "        \n",
    "        return cumulative_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------causal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiheadAttentionWithRoPE(nn.MultiheadAttention):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, bias=True, add_bias_kv=False,\n",
    "                 add_zero_attn=False, kdim=None, vdim=None, base=10000, batch_first=True):\n",
    "        super(MultiheadAttentionWithRoPE, self).__init__(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=add_zero_attn,\n",
    "            kdim=kdim,\n",
    "            vdim=vdim,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.rotary_emb = RotaryPositionalEmbeddingROPE(self.head_dim, base=base)\n",
    "    \n",
    "    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None, need_weights=True,\n",
    "                average_attn_weights=True, is_causal=True, **kwargs):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_len, _ = query.size()\n",
    "            # Reshape and apply RoPE\n",
    "            Q = query.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            K = key.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            Q = self.rotary_emb(Q)\n",
    "            K = self.rotary_emb(K)\n",
    "            # Reshape back to original shape\n",
    "            Q = Q.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "            K = K.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        else:\n",
    "            raise NotImplementedError(\"batch_first=False is not supported in this custom attention.\")\n",
    "\n",
    "        if is_causal:\n",
    "            print(\"in\")\n",
    "            # Create causal mask as a boolean tensor\n",
    "            causal_mask = torch.triu(torch.ones((seq_len, seq_len), device=query.device, dtype=torch.bool), diagonal=1)\n",
    "            if attn_mask is not None:\n",
    "                print(\"not none\")\n",
    "                # Combine with existing attn_mask if provided\n",
    "                attn_mask = attn_mask.logical_or(causal_mask)\n",
    "            else:\n",
    "                print(\"else\")\n",
    "                attn_mask = causal_mask\n",
    "            # No need to convert attn_mask dtype; boolean is acceptable\n",
    "        # Ensure attn_mask has the correct shape and dtype\n",
    "        # For batch_first=True, attn_mask should be [seq_len, seq_len]\n",
    "\n",
    "        # Call the parent class's forward method\n",
    "        attn_output, attn_weights = super(MultiheadAttentionWithRoPE, self).forward(\n",
    "            Q, K, value, key_padding_mask=key_padding_mask, attn_mask=attn_mask,\n",
    "            need_weights=need_weights, average_attn_weights=average_attn_weights\n",
    "        )\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class TransformerEncoderLayerWithRoPE(TransformerEncoderLayer):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1, base=10000, activation='relu', batch_first=True):\n",
    "        super(TransformerEncoderLayerWithRoPE, self).__init__(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        self.self_attn = MultiheadAttentionWithRoPE(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            base=base,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "        # Initialize other layers if needed\n",
    "        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Pass is_causal=True to self_attn\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask, is_causal=True)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ef856a-ac56-413c-a005-292ee022d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sequence_length(X_batch, y_batch, max_positive_adjust=8, max_negative_adjust=2, pad_value=0, pad_label=5):\n",
    "    \"\"\"\n",
    "    Adjusts the length of all sequences in the batch by a randomly sampled delta within specified positive and negative adjustment limits.\n",
    "    Ensures that no sequence becomes fully padded.\n",
    "\n",
    "    Args:\n",
    "        X_batch (torch.Tensor): Input sequences [batch_size, seq_len, input_dim].\n",
    "        y_batch (torch.Tensor): Label sequences [batch_size, seq_len].\n",
    "        max_positive_adjust (int): Maximum number of timesteps to extend sequences.\n",
    "        max_negative_adjust (int): Maximum number of timesteps to shorten sequences.\n",
    "        pad_value (int/float): Value used for padding in input sequences.\n",
    "        pad_label (int): Label index used for padding in label sequences.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor, torch.Tensor: Adjusted input and label sequences \n",
    "                                     [new_batch_size, new_seq_len, input_dim], [new_batch_size, new_seq_len].\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, input_dim = X_batch.shape\n",
    "\n",
    "    # Sample a single delta for the entire batch within the allowed range\n",
    "    delta = random.randint(-max_negative_adjust, max_positive_adjust)\n",
    "\n",
    "    # Determine the new sequence length\n",
    "    new_seq_len = seq_len + delta\n",
    "    new_seq_len = max(new_seq_len, 1)  # Ensure at least 1 timestep remains\n",
    "\n",
    "    # Initialize padding or truncation based on delta\n",
    "    if delta > 0:\n",
    "        # Extend the sequences by padding at the beginning\n",
    "        padding_X = torch.full((batch_size, delta, input_dim), pad_value, device=X_batch.device, dtype=X_batch.dtype)\n",
    "        padding_y = torch.full((batch_size, delta), pad_label, device=y_batch.device, dtype=y_batch.dtype)\n",
    "        adjusted_X = torch.cat((padding_X, X_batch), dim=1)[:, :new_seq_len, :]  # Pad at the beginning and trim\n",
    "        adjusted_y = torch.cat((padding_y, y_batch), dim=1)[:, :new_seq_len]\n",
    "    elif delta < 0:\n",
    "        # Shorten the sequences by truncating from the end\n",
    "        adjusted_X = X_batch[:, :new_seq_len, :]\n",
    "        adjusted_y = y_batch[:, :new_seq_len]\n",
    "    else:\n",
    "        # No adjustment needed\n",
    "        adjusted_X = X_batch\n",
    "        adjusted_y = y_batch\n",
    "\n",
    "    # Identify sequences that have all timesteps as padding\n",
    "    is_all_padded = (adjusted_y == pad_label).all(dim=1)  # [batch_size]\n",
    "\n",
    "    # Remove fully padded sequences from the batch\n",
    "    if is_all_padded.any():\n",
    "        num_removed = is_all_padded.sum().item()\n",
    "        #print(f\"Removing {num_removed} fully-padded sequences from the batch.\")\n",
    "        adjusted_X = adjusted_X[~is_all_padded]\n",
    "        adjusted_y = adjusted_y[~is_all_padded]\n",
    "\n",
    "    return adjusted_X, adjusted_y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "382aae01-df42-45ce-89d0-0909beb8b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sacmh(model, train_loader, val_loader, sac_loss, \n",
    "                      optimizer, reward_baja_2, penalty_baja_2, miss_baja_2_penalty, num_epochs, \n",
    "                      pad_value=0, device='cuda'):#alpha_optimizer,\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Initialize variables to track the best metrics\n",
    "    best_val_loss = float('inf')\n",
    "    best_ganancia_epoch = float('-inf')\n",
    "    best_argmax_ganancia_epoch = float('-inf')\n",
    "    best_threshold_ganancia_epoch = float('-inf')\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=5, threshold=0.00001)\n",
    "\n",
    "    ganancia_values = []\n",
    "    ganancia_argmax_values = []\n",
    "    ganancia_threshold_values = []\n",
    "   \n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    \n",
    "    best_threshold_values = []\n",
    "    threshold1_values = []\n",
    "    threshold2_values = []\n",
    "    thresholdc_values = []\n",
    "\n",
    "    threshold1_values_best = []\n",
    "    threshold2_values_best = []\n",
    "    thresholdc_values_best = []\n",
    "    \n",
    "    epochs_list = []\n",
    "    actor_loss_values = []\n",
    "    critic_loss_values = []\n",
    "    entropy_loss_values = []\n",
    "    use_thresholds = None\n",
    "    \n",
    "    threshold_learning_rate = 0.001\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        epochs_list.append(epoch + 1) \n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_loss_actor = 0.0\n",
    "        train_loss_critic = 0.0\n",
    "        train_loss_entropy = 0.0\n",
    "\n",
    "        # Use tqdm to wrap the training loop\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True, ncols=140)\n",
    "\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            X_batch, y_batch = batch\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            X_batch, y_batch = adjust_sequence_length(X_batch, y_batch, max_positive_adjust=8, max_negative_adjust=2, pad_value=pad_value, pad_label=5)\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            y_batch = y_batch.squeeze(-1)\n",
    "            mask = create_padding_mask(X_batch, pad_value).to(device)  # Boolean mask: True for padding positions\n",
    "            mask_float = (~mask).float()  # Float mask: 1.0 for valid positions, 0.0 for padding\n",
    "            # Identify sequences that are not fully padded\n",
    "            all_padded = mask.all(dim=1)  # True if the sequence is all padding\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=True)[0]  # 1D tensor of indices\n",
    "        \n",
    "            if non_empty_indices.numel() == 0:\n",
    "                # Skip this batch if all sequences are fully padded\n",
    "                print(\"All sequences in the batch are fully padded. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            #print(\"mask input\")\n",
    "            #print(mask)\n",
    "            action_probs = model(X_batch, src_key_padding_mask=mask)\n",
    "\n",
    "             # Compute class predictions\n",
    "            with torch.no_grad():\n",
    "                if use_thresholds != None:\n",
    "                    predicted_labels = get_class_labels_best(action_probs, thresholds=best_thresholds)\n",
    "                else:\n",
    "                    predicted_labels = get_class_labels(action_probs, threshold=0.5)\n",
    "\n",
    "            # Compute rewards based on predictions and labels\n",
    "            rewards = compute_rewards_ordinal(preds=predicted_labels,labels=y_batch, mask=mask_float, device=device)  # [batch_size, seq_len]\n",
    "\n",
    "            # Compute loss using rewards\n",
    "            total_loss = sac_loss(action_probs=action_probs, target_labels=y_batch, rewards=rewards, mask=mask_float,class_weights=None)\n",
    "        \n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "        \n",
    "            # Optional: Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()       \n",
    "\n",
    "            progress_bar.set_postfix({\"TLoss\": abs(train_loss / len(train_loader))})\n",
    "            \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, best_ganancia, best_argmax_ganancia, best_thresholds, best_ganancia_threshold, best_threshold_c = validation_loop_sacmh(model, val_loader,\n",
    "                                                                         sac_loss, pad_value, device, use_thresholds, miss_baja_2_penalty, \n",
    "                                                                         reward_baja_2, penalty_baja_2)\n",
    "        if use_thresholds == None:    \n",
    "            use_thresholds = best_thresholds\n",
    "        \n",
    "        \n",
    "        t_loss = abs(train_loss)\n",
    "        \n",
    "        # Check if there were improvements\n",
    "        val_loss_improved = abs(val_loss) < abs(best_val_loss)\n",
    "        ganancia_improved = best_ganancia > best_ganancia_epoch\n",
    "        ganancia_argmax_improved = best_argmax_ganancia > best_argmax_ganancia_epoch\n",
    "        threshold_ganancia_improved = best_ganancia_threshold > best_threshold_ganancia_epoch\n",
    "        \n",
    "        ganancia_values.append(best_ganancia)\n",
    "        ganancia_argmax_values.append(best_argmax_ganancia)\n",
    "        ganancia_threshold_values.append(best_ganancia_threshold)\n",
    "\n",
    "        best_threshold_values.append(best_thresholds)\n",
    "        \n",
    "        train_loss_values.append(t_loss)\n",
    "        val_loss_values.append(abs(val_loss))\n",
    "        \n",
    "           \n",
    "\n",
    "        # Update best_val_loss and best_ganancia_epoch if improved\n",
    "        if val_loss_improved:\n",
    "            best_val_loss = val_loss\n",
    "            model_name = f\"best_model_val_loss_{true_model}.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f'Saving model with best validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "        if ganancia_improved:\n",
    "            best_ganancia_epoch = best_ganancia\n",
    "            model_name = f\"best_model_ganancia_{true_model}.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f'Saving model with best ganancia: {best_ganancia_epoch}')\n",
    "\n",
    "        if ganancia_argmax_improved:\n",
    "            best_argmax_ganancia_epoch = best_argmax_ganancia\n",
    "            model_name = f\"best_model_argmax_ganancia_{true_model}.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f'Saving model with best argmax ganancia: {best_argmax_ganancia_epoch}')\n",
    "\n",
    "        if threshold_ganancia_improved:\n",
    "            best_threshold_ganancia_epoch = best_ganancia_threshold\n",
    "            model_name = f\"best_model_threshold_ganancia_{true_model}.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f'Saving model with best threshold ganancia: {best_threshold_ganancia_epoch}')\n",
    "            print(f\"Changing to thresholds: {best_thresholds}\")\n",
    "            use_thresholds = best_thresholds\n",
    "\n",
    "        # Step the scheduler only if there were no improvements in both metrics\n",
    "        if not val_loss_improved and not ganancia_improved and not ganancia_argmax_improved and not threshold_ganancia_improved:\n",
    "            print(\"LR Stepping in!\")\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(\"\")\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], T_Loss: {t_loss:.4f}, V_Loss: {val_loss:.4f}, Ganancia: {best_ganancia}, argmaxG: {best_argmax_ganancia}, LR: {current_lr}')\n",
    "\n",
    "        if print_plot:\n",
    "            plot_training_metrics(epochs_list, train_loss_values, val_loss_values, ganancia_values, ganancia_argmax_values,\n",
    "                                  ganancia_threshold_values, best_threshold_values, entropy_loss_values, threshold1_values_best, threshold2_values_best, thresholdc_values_best, threshold1_values, threshold2_values, thresholdc_values)\n",
    "\n",
    "    print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15dd6c52-0d0c-4427-867f-af9310d98bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from indices to class labels\n",
    "index_to_class = {\n",
    "    0: 'BAJA+1',\n",
    "    1: 'BAJA+2',\n",
    "    2: 'BAJA+3',\n",
    "    3: 'BAJA+4',\n",
    "    4: 'CONTINUA',\n",
    "    5: 'OUT'\n",
    "}\n",
    "\n",
    "# Function to map indices to class labels\n",
    "def map_indices_to_classes(nearest_label_indices):\n",
    "    # Ensure indices are within [0, 5]\n",
    "    nearest_label_indices = nearest_label_indices.clamp(0, 5)\n",
    "    # Vectorize the mapping\n",
    "    class_labels = torch.tensor([index_to_class[idx.item()] for idx in nearest_label_indices.flatten()], device=nearest_label_indices.device)\n",
    "    class_labels = class_labels.view(nearest_label_indices.shape)\n",
    "    return class_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31328f4d-a9c0-47a7-a4eb-9b6f7c619cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop_sacmh(model, val_loader, sac_loss, pad_value, device, use_thresholds,\n",
    "                          miss_baja_2_penalty, reward_baja_2, penalty_baja_2, \n",
    "                          baja_1_thresholds=np.arange(0.0, 1.0, 0.05), \n",
    "                          baja_2_thresholds=np.arange(0.0, 1.0, 0.05), \n",
    "                          c_thresholds=np.arange(0.0, 1.0, 0.05),\n",
    "                          val_steps=2,\n",
    "                          print_class_report=print_class_report,\n",
    "                          print_class_count=print_class_count):\n",
    "    val_loss = 0.0\n",
    "    actor_loss_item = 0.0\n",
    "    critic_loss_item = 0.0\n",
    "    \n",
    "    all_preds_last2 = []\n",
    "    all_labels_last2 = []\n",
    "    all_probs_last2 = []\n",
    "\n",
    "    all_preds_last2_all = []\n",
    "    all_labels_last2_all = []\n",
    "    all_probs_last2_all = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=\"Validation\", leave=True, ncols=140)\n",
    "\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            X_val_batch, y_val_batch = batch\n",
    "        \n",
    "            # Move validation data to the device\n",
    "            X_val_batch = X_val_batch.to(device)\n",
    "            y_val_batch = y_val_batch.to(device)\n",
    "\n",
    "            y_val_batch = y_val_batch.squeeze(-1)  # [batch_size, seq_len]\n",
    "\n",
    "            batch_size, seq_len, _ = X_val_batch.shape    \n",
    "\n",
    "            mask = create_padding_mask(X_val_batch, pad_value).to(device)\n",
    "            mask_float = (~mask).float()\n",
    "    \n",
    "            all_padded = mask.all(dim=1)\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=False).squeeze()\n",
    "\n",
    "            if non_empty_indices.numel() == 0:\n",
    "                print(\"All sequences in the batch are fully padded. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            X_val_batch = X_val_batch[non_empty_indices]\n",
    "            y_val_batch = y_val_batch[non_empty_indices]\n",
    "            mask = mask[non_empty_indices]\n",
    "            mask_float = mask_float[non_empty_indices]\n",
    "\n",
    "  \n",
    "\n",
    "            action_probs = model(X_val_batch, src_key_padding_mask=mask)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if use_thresholds != None:\n",
    "                    predicted_labels = get_class_labels_best(action_probs, thresholds=use_thresholds)\n",
    "                else:   \n",
    "                    predicted_labels = get_class_labels(action_probs, threshold=0.5)\n",
    "\n",
    "\n",
    "            # Compute rewards based on predictions and labels\n",
    "            rewards = compute_rewards_ordinal(preds=predicted_labels,labels=y_val_batch, mask=mask_float, device=device)  # [batch_size, seq_len]\n",
    "\n",
    "            # Compute loss using rewards\n",
    "            total_loss = sac_loss(action_probs=action_probs, target_labels=y_val_batch, rewards=rewards, mask=mask_float, class_weights=None)\n",
    "\n",
    "            # Accumulate the losses for reporting\n",
    "            actor_loss_item += total_loss\n",
    "\n",
    "            # Slice outputs and targets for the last 'val_steps' timesteps\n",
    "            val_outputs_last2 = predicted_labels[:, -val_steps:]  # Shape: [batch_size, val_steps, num_classes]\n",
    "            y_val_batch_last2 = y_val_batch[:, -val_steps:]    # Shape: [batch_size, val_steps]\n",
    "\n",
    "            # Filter out the OUT class (class value 69)\n",
    "            valid_indices = (y_val_batch_last2 != 5) # Boolean mask\n",
    "            valid_indices_all = (y_val_batch != 5)\n",
    "            \n",
    "            # Apply the mask to filter out the OUT class\n",
    "            filtered_last_timestep_probs = val_outputs_last2[valid_indices]  # Shape: [num_valid_samples, 1]\n",
    "            filtered_last_timestep_labels = y_val_batch_last2[valid_indices]     # Shape: [num_valid_samples]\n",
    "\n",
    "            filtered_last_timestep_probs_all = predicted_labels[valid_indices_all]  # Shape: [num_valid_samples, 1]\n",
    "            filtered_last_timestep_labels_all = y_val_batch[valid_indices_all]     # Shape: [num_valid_samples]\n",
    "\n",
    "            # Collect predicted probabilities and labels\n",
    "            all_probs_last2.append(filtered_last_timestep_probs)\n",
    "            all_labels_last2.append(filtered_last_timestep_labels)\n",
    "\n",
    "            # Collect predicted probabilities and labels\n",
    "            all_probs_last2_all.append(filtered_last_timestep_probs_all)\n",
    "            all_labels_last2_all.append(filtered_last_timestep_labels_all)\n",
    "\n",
    "            # Update progress bar or logging\n",
    "            progress_bar.set_postfix({\n",
    "                \"VLoss\": (abs(actor_loss_item)) / len(val_loader)\n",
    "            })\n",
    "\n",
    "        \n",
    "        # Finalize loss calculations\n",
    "        actor_loss_item /= len(val_loader)\n",
    "        \n",
    "        val_loss = abs(actor_loss_item)\n",
    "\n",
    "        # Convert lists to arrays for threshold sweep\n",
    "        if all_probs_last2:\n",
    "            all_probs_last2 = [tensor.cpu().numpy() for tensor in all_probs_last2]\n",
    "            all_probs_last2 = np.concatenate(all_probs_last2, axis=0)\n",
    "\n",
    "            all_probs_last2_all = [tensor.cpu().numpy() for tensor in all_probs_last2_all]\n",
    "            all_probs_last2_all = np.concatenate(all_probs_last2_all, axis=0)\n",
    "        else:\n",
    "            all_probs_last2 = np.array([])\n",
    "            print(\"\\nNo valid prediction probabilities collected.\")\n",
    "\n",
    "        if all_labels_last2:\n",
    "            all_labels_last2 = [tensor.cpu().numpy() for tensor in all_labels_last2]\n",
    "            all_labels_last2 = np.concatenate(all_labels_last2, axis=0)\n",
    "\n",
    "            all_labels_last2_all = [tensor.cpu().numpy() for tensor in all_labels_last2_all]\n",
    "            all_labels_last2_all = np.concatenate(all_labels_last2_all, axis=0)\n",
    "        else:\n",
    "            all_labels_last2 = np.array([])\n",
    "            print(\"No valid labels collected.\")\n",
    "\n",
    "        # Perform threshold sweep to find the best threshold\n",
    "        if all_probs_last2.size > 0 and all_labels_last2.size > 0:\n",
    "            ganancia, argmax_ganancia, _, _, _, preds = calculate_best_threshold_ganancia(\n",
    "                all_probs_last2, all_labels_last2, baja_2_index=1\n",
    "            )\n",
    "            ganancia_all, argmax_ganancia_all, _, _, _, preds = calculate_best_threshold_ganancia(\n",
    "                all_probs_last2_all, all_labels_last2_all, baja_2_index=1\n",
    "            )\n",
    "            if use_thresholds != None:   \n",
    "                print(f\"\\nGananciaALL: {ganancia_all}, GananciaLast: {ganancia}, Thresholds: {use_thresholds}\")\n",
    "            else:\n",
    "                print(f\"\\nGananciaALL: {ganancia_all}, GananciaLast: {ganancia}\")\n",
    "        else:\n",
    "            ganancia, argmax_ganancia, preds = 0.0, 0.0, np.array([])\n",
    "            print(\"\\nNo valid predictions or labels to calculate ganancia.\")\n",
    "        \n",
    "\n",
    "        # Optionally, print classification report after all batches are processed\n",
    "        if print_class_report and preds.size > 0 and all_labels_last2.size > 0:\n",
    "            class_labels = [0, 1, 2, 3, 4, 5]\n",
    "            target_names = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA', 'OUT']\n",
    "            \n",
    "            print(\"\\nClassification Report for LAST Validation Set:\")\n",
    "            print(classification_report(\n",
    "                all_labels_last2,  # y_true: Ground truth labels\n",
    "                all_probs_last2,            # y_pred: Predicted class labels\n",
    "                labels=class_labels,  \n",
    "                target_names=target_names,\n",
    "                zero_division=0  \n",
    "            ))\n",
    "            print(\"\\nClassification Report for ALL Validation Set:\")\n",
    "            print(classification_report(\n",
    "                all_labels_last2_all,  # y_true: Ground truth labels\n",
    "                all_probs_last2_all,            # y_pred: Predicted class labels\n",
    "                labels=class_labels,  \n",
    "                target_names=target_names,\n",
    "                zero_division=0  \n",
    "            ))\n",
    "\n",
    "        # Optionally, print class counts after all batches are processed\n",
    "        if print_class_count and preds.size > 0:\n",
    "            class_labels_list = [0, 1, 2, 3, 4, 5]\n",
    "            target_names = ['BAJA+1', 'BAJA+2', 'BAJA+3', 'BAJA+4', 'CONTINUA', 'OUT']\n",
    "            \n",
    "            class_counts = Counter(all_probs_last2)\n",
    "            print(\"\\nFinal Class Counts (Predicted for Last/val Timesteps):\")\n",
    "            for class_value, class_name in zip(class_labels_list, target_names):\n",
    "                count = class_counts.get(class_value, 0)\n",
    "                print(f\"{class_name} ({class_value}): {count}\")\n",
    "            print(\"\")\n",
    "\n",
    "            class_counts = Counter(all_probs_last2_all)\n",
    "            print(\"\\nFinal Class Counts (Predicted for ALL Timesteps):\")\n",
    "            for class_value, class_name in zip(class_labels_list, target_names):\n",
    "                count = class_counts.get(class_value, 0)\n",
    "                print(f\"{class_name} ({class_value}): {count}\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "        \n",
    "        best_thresholds, best_ganancia_threshold = threshold_tuning_vectorized(model, val_loader, compute_rewards_ordinal_vectorized, pad_value, device=device, max_combinations_per_batch=5000)\n",
    "\n",
    "        \n",
    "          \n",
    "        \n",
    "        return val_loss, ganancia, ganancia_all, best_thresholds, best_ganancia_threshold, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "540b1ef8-790c-435f-a9e1-4b3a43f50f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Classes: [ 1  2  3  4  5 69]\n",
      "Class to Index Mapping: {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 69}\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train, X_val, y_val, X_test, y_test = split_data(sequence_array, label_array, train_steps, val_steps, test_steps)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(sequence_array, label_array, train_steps=train_steps, val_steps=val_steps, \n",
    "                                                            test_steps=test_steps, split_by_clients=split_by_clients, client_split_ratios=client_split_ratios)\n",
    "\n",
    "# Encode labels for training, validation, and testing\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train.flatten()).reshape(y_train.shape)\n",
    "y_val_encoded = label_encoder.transform(y_val.flatten()).reshape(y_val.shape)\n",
    "y_test_encoded = label_encoder.transform(y_test.flatten()).reshape(y_test.shape)\n",
    "\n",
    "# Print encoded class names and their mappings (optional for debugging)\n",
    "class_names = label_encoder.classes_\n",
    "class_mapping = {idx: class_name for idx, class_name in enumerate(class_names)}\n",
    "print(f\"Encoded Classes: {class_names}\")\n",
    "print(f\"Class to Index Mapping: {class_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2776daa8-6c0c-49c7-95dd-5ccc27b97624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([146972, 29, 306])\n",
      "y_train shape: torch.Size([146972, 29])\n",
      "X_val_tensor shape: torch.Size([36743, 31, 306])\n",
      "y_val_tensor shape: torch.Size([36743, 31])\n",
      "X_test_tensor shape: torch.Size([0])\n",
      "y_test_tensor shape: torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "# Convert training, validation, and test data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)  # Ensure labels are long (int64)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)  # Ensure labels are long (int64)\n",
    "\n",
    "# Resulting shapes after undersampling\n",
    "print(f\"X_train shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train shape: {y_train_tensor.shape}\")\n",
    "\n",
    "print(f\"X_val_tensor shape: {X_val_tensor.shape}\")\n",
    "print(f\"y_val_tensor shape: {y_val_tensor.shape}\")\n",
    "\n",
    "print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "print(f\"y_test_tensor shape: {y_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04859435-3090-40bf-9fad-35592a55abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REDUCE:\n",
    "    # Slice the tensors to keep only the first 'num_clients' rows\n",
    "    X_test_tensor = X_test_tensor[:num_clients]\n",
    "    y_test_tensor = y_test_tensor[:num_clients]\n",
    "    \n",
    "    X_train_tensor = X_train_tensor[:num_clients]\n",
    "    y_train_tensor = y_train_tensor[:num_clients]\n",
    "\n",
    "    print(\"REDUCED\")\n",
    "    \n",
    "    X_val_tensor = X_val_tensor[:num_clients]\n",
    "    y_val_tensor = y_val_tensor[:num_clients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed9d6b4-d572-46c7-ada7-4061478a84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UNDERSAMPLE:   \n",
    "    # Identify clients where all timesteps are the majority class or the padding label\n",
    "    clients_all_majority_or_padding = ((y_train_tensor == majority_class) | (y_train_tensor == padding_label)).all(dim=1)\n",
    "    \n",
    "    # Get indices of clients who are either all majority class or majority + padding\n",
    "    majority_or_padding_indices = clients_all_majority_or_padding.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Identify clients that belong to minority classes or have mixed labels\n",
    "    clients_not_all_majority_or_padding = (~clients_all_majority_or_padding).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    # Determine how many clients to undersample based on the percentage\n",
    "    undersample_size = int(len(majority_or_padding_indices) * undersample_percentage)\n",
    "    \n",
    "    # Perform undersampling: randomly select a percentage of clients with all majority class or majority + padding\n",
    "    undersampled_majority_or_padding_indices = np.random.choice(majority_or_padding_indices.cpu(), size=undersample_size, replace=False)\n",
    "    \n",
    "    # Combine undersampled majority + padding clients with minority or mixed label clients\n",
    "    final_indices = torch.cat((torch.tensor(undersampled_majority_or_padding_indices), clients_not_all_majority_or_padding))\n",
    "    \n",
    "    # Subset the data using the selected indices\n",
    "    X_train_tensor = X_train_tensor[final_indices]\n",
    "    y_train_tensor = y_train_tensor[final_indices]\n",
    "\n",
    "    print(\"UNDERSAMPLED\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34ce16f3-ed17-41a8-b083-bfdd4c5f67a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([146972, 29, 306])\n",
      "y_train shape: torch.Size([146972, 29])\n",
      "X_val_tensor shape: torch.Size([36743, 31, 306])\n",
      "y_val_tensor shape: torch.Size([36743, 31])\n",
      "X_test_tensor shape: torch.Size([0])\n",
      "y_test_tensor shape: torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "# Resulting shapes after undersampling\n",
    "print(f\"X_train shape: {X_train_tensor.shape}\")\n",
    "print(f\"y_train shape: {y_train_tensor.shape}\")\n",
    "\n",
    "print(f\"X_val_tensor shape: {X_val_tensor.shape}\")\n",
    "print(f\"y_val_tensor shape: {y_val_tensor.shape}\")\n",
    "\n",
    "print(f\"X_test_tensor shape: {X_test_tensor.shape}\")\n",
    "print(f\"y_test_tensor shape: {y_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04646dc7-3e4f-428b-ba12-646871f5988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [47.97492177 47.78130535 47.0128833  45.95747342  0.21407649  0.80387821]\n"
     ]
    }
   ],
   "source": [
    "padding_label = 69  # The label used for padding\n",
    "num_classes = 5  # Number of classes excluding the padding label\n",
    "\n",
    "# Flatten the label tensor to 1D: [batch_size * seq_len]\n",
    "y_train_flat = y_train_tensor.flatten()\n",
    "\n",
    "# Exclude the padding label\n",
    "y_train_flat_no_padding = y_train_flat[y_train_flat != padding_label]\n",
    "\n",
    "# Convert to numpy array for use with sklearn\n",
    "y_train_np = y_train_flat_no_padding.cpu().numpy()  # Convert to numpy array (if it's on a GPU)\n",
    "\n",
    "# Compute class weights using sklearn's compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "\n",
    "# Output class weights (you can convert it to tensor later)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Free up memory by deleting unnecessary variables\n",
    "del y_train_flat, y_train_flat_no_padding, y_train_np\n",
    "gc.collect()  # Run garbage collector to free up memory\n",
    "\n",
    "# If using GPU, release cached GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b82020f4-9438-4196-bf09-e13cc979bf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 1, 0, 5, 5,\n",
       "        5, 5, 4, 4, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor[2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07c7ec0e-7ee6-4393-bc16-ba20b0ab6e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845092e-69a8-491d-b9a9-18f8f3269c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a344618d-5e54-4ca3-9547-9d0cd56b1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "198569ef-8246-43ed-a5c2-df3977cf0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not runOPTUNA:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57d3ee-bb86-44a4-b9f0-426e34f941d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e51eb-21b8-4ac1-b1b3-f589ff291a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By lem0n\n",
      "-----------------\n",
      "Using: cuda\n",
      "Using weights: tensor([60., 70.,  1.], device='cuda:0')\n",
      "Using d_model: 500\n",
      "Using dim_feedforward: 1000\n",
      "Using dropout: 0.1\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|| 368/368 [00:54<00:00,  6.72it/s, TLoss=0.0345]\n",
      "Validation: 100%|| 184/184 [00:05<00:00, 32.66it/s, VLoss=tensor(0.0291, device='cuda:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GananciaALL: -1997364, GananciaLast: -124065\n",
      "\n",
      "Classification Report for LAST Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BAJA+1       0.00      0.00      0.00       395\n",
      "      BAJA+2       0.01      0.99      0.02       495\n",
      "      BAJA+3       0.00      0.01      0.00       256\n",
      "      BAJA+4       0.00      0.00      0.00         0\n",
      "    CONTINUA       0.00      0.00      0.00     64663\n",
      "         OUT       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.01      0.01      0.01     65809\n",
      "   macro avg       0.00      0.17      0.00     65809\n",
      "weighted avg       0.00      0.01      0.00     65809\n",
      "\n",
      "\n",
      "Classification Report for ALL Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BAJA+1       0.00      0.00      0.00      4079\n",
      "      BAJA+2       0.01      0.99      0.01      4200\n",
      "      BAJA+3       0.00      0.01      0.00      4015\n",
      "      BAJA+4       0.00      0.00      0.00      3848\n",
      "    CONTINUA       0.00      0.00      0.00    895378\n",
      "         OUT       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00    911520\n",
      "   macro avg       0.00      0.17      0.00    911520\n",
      "weighted avg       0.00      0.00      0.00    911520\n",
      "\n",
      "\n",
      "Final Class Counts (Predicted for Last/val Timesteps):\n",
      "BAJA+1 (0): 0\n",
      "BAJA+2 (1): 60995\n",
      "BAJA+3 (2): 4739\n",
      "BAJA+4 (3): 75\n",
      "CONTINUA (4): 0\n",
      "OUT (5): 0\n",
      "\n",
      "\n",
      "Final Class Counts (Predicted for ALL Timesteps):\n",
      "BAJA+1 (0): 0\n",
      "BAJA+2 (1): 832868\n",
      "BAJA+3 (2): 77576\n",
      "BAJA+4 (3): 1076\n",
      "CONTINUA (4): 0\n",
      "OUT (5): 0\n",
      "\n",
      "Processed chunk 1/5, Best Ganancia in chunk: 8037.0 Ts: (0.01, 0.28, 0.28, 0.28)\n",
      "Processed chunk 2/5, Best Ganancia in chunk: 8037.0 Ts: (0.06999999999999999, 0.28, 0.28, 0.28)\n",
      "Processed chunk 3/5, Best Ganancia in chunk: 8037.0 Ts: (0.13, 0.28, 0.28, 0.28)\n",
      "Processed chunk 4/5, Best Ganancia in chunk: 8037.0 Ts: (0.25, 0.28, 0.28, 0.28)\n",
      "Processed chunk 5/5, Best Ganancia in chunk: -120081.0 Ts: (0.49, 0.49, 0.55, 0.64)\n",
      "\n",
      "Selected Best Thresholds after maximizing F1-score for BAJA+2:\n",
      "Thresholds: (0.14500000000000016, 0.28000000000000064, 0.28000000000000064, 0.5050000000000002), F1-score: 0.06779661029577255, Ganancia: 8037.0\n",
      "\n",
      "Classification Report for Validation Set with Selected Best Thresholds:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BAJA+1       0.00      0.00      0.00       395\n",
      "      BAJA+2       0.04      0.42      0.07       495\n",
      "      BAJA+3       0.01      0.60      0.01       256\n",
      "      BAJA+4       0.00      0.00      0.00         0\n",
      "    CONTINUA       0.00      0.00      0.00     64663\n",
      "\n",
      "    accuracy                           0.01     65809\n",
      "   macro avg       0.01      0.20      0.02     65809\n",
      "weighted avg       0.00      0.01      0.00     65809\n",
      "\n",
      "Saving model with best validation loss: 0.0291\n",
      "Saving model with best ganancia: -124065\n",
      "Saving model with best argmax ganancia: -1997364\n",
      "Saving model with best threshold ganancia: 8037.0\n",
      "Changing to thresholds: (0.14500000000000016, 0.28000000000000064, 0.28000000000000064, 0.5050000000000002)\n",
      "\n",
      "Epoch [1/300], T_Loss: 0.0345, V_Loss: 0.0291, Ganancia: -124065, argmaxG: -1997364, LR: 5e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300:   9%|                                                                   | 34/368 [00:05<00:52,  6.42it/s, TLoss=0.00367]"
     ]
    }
   ],
   "source": [
    "print(\"By lem0n\")\n",
    "print(\"-----------------\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using: \" + device)\n",
    "\n",
    "if useTRANSFORMERS:\n",
    "    model = TransformerModel(input_dim=input_dim, d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: Transformers\")\n",
    "\n",
    "elif useTRANSFORMERSHYBRID:\n",
    "    model = TransformerModelHybrid(input_dim=input_dim, d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: TransformersHybrid\")\n",
    "\n",
    "elif useACTORCRITIC:\n",
    "    model = ActorCriticTransformerModel(input_dim=input_dim, d_model=d_model, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: Transformers Actor-Critic\")\n",
    "\n",
    "elif useLSTM:\n",
    "    model = LSTMModel(input_dim=input_dim, d_model=d_model, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: LSTM\")\n",
    "\n",
    "elif useLSTMAttention:\n",
    "    model = LSTMModelWithAttention(input_dim=input_dim, d_model=d_model, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: LSTMAttention\")\n",
    "\n",
    "elif useLSTMWeightAtention:\n",
    "    model = LSTMModelWithAttentionWeight(input_dim=input_dim, d_model=d_model, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: LSTMWeight\")\n",
    "\n",
    "elif useLSTMEncoder:\n",
    "    model = LSTMModelWithLearnablePositionalEncodings(input_dim=input_dim, d_model=d_model, num_layers=num_layers, num_classes=num_classes, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "    print(\"Model: LSTMEncoder\")\n",
    "\n",
    "\n",
    "\n",
    "if not runOPTUNA:\n",
    "    if setCLASSWEIGHTS:   \n",
    "        class_weights = torch.tensor(class_weights_vector, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"Using weights: {class_weights}\")\n",
    "print(f\"Using d_model: {d_model}\")\n",
    "print(f\"Using dim_feedforward: {dim_feedforward}\")\n",
    "print(f\"Using dropout: {dropout}\")\n",
    "\n",
    "\n",
    "print(\"-----------------\")\n",
    "\n",
    "if not runOPTUNA:\n",
    "\n",
    "\n",
    "    if useSACMH:\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "        model = ActorCriticOrdinalTransformerModel(\n",
    "            input_dim=input_dim,  # Replace with your input dimension\n",
    "            #d_model=512,\n",
    "            d_model=512,\n",
    "            num_heads=4,\n",
    "            #num_heads=8,\n",
    "            #num_layers=6,\n",
    "            num_layers=4,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.2,\n",
    "            num_actor_heads=4,\n",
    "            #num_actor_heads=10,\n",
    "            num_classes=5,\n",
    "            base=10000\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        class_weights = torch.tensor([50.0, 50.0, 50.0, 50.0, 1, 1], device=device)\n",
    "        sac_loss = RewardWeightedBCEOrdinalLoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Define learning rates\n",
    "        actor_lr = 5e-6\n",
    "        critic_lr = 5e-4 #antes -5\n",
    "        actor_weight_decay = 1e-7  # Example weight decay value for actor\n",
    "        critic_weight_decay = 1e-7  # Example weight decay value for critic\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_model_sacmh(\n",
    "            model, \n",
    "   #         critic_network,  # Pass the critic network\n",
    "  #          target_critic_network,  # Pass the target critic network\n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            sac_loss,  # Loss function with SAC logic\n",
    "            optimizer,  # Optimizer for the actor (policy network)\n",
    "  #          critic_optimizer,  # Optimizer for the critic network\n",
    "            #alpha_optimizer,  # Optimizer for the learnable alpha parameter\n",
    "            num_epochs=num_epochs, \n",
    "            pad_value=0,  # Ensure the pad value is passed (if needed)\n",
    "            device=device,\n",
    "            miss_baja_2_penalty=miss_baja_2_penalty, \n",
    "            reward_baja_2=reward_baja_2, \n",
    "            penalty_baja_2=penalty_baja_2\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim_mod.Yogi(model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        if useACTORCRITIC:\n",
    "            if withSOFTDISCOUNT:\n",
    "                criterion = ActorCriticGananciaLoss(class_weights=class_weights, gamma = gamma).to(device)\n",
    "                print(\"Using ActorLoss with SOFT DISCOUNTS\")\n",
    "            else:\n",
    "                criterion = ActorCriticGananciaLossSoft(class_weights=class_weights, gamma = gamma).to(device)\n",
    "                print(\"Using ActorLoss with SOFT\")     \n",
    "        else:\n",
    "            criterion = CustomGananciaLoss(class_weights=class_weights, gamma = gamma).to(device)\n",
    "            print(\"Using GananciaLoss\")\n",
    "    \n",
    "        \n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer, miss_baja_2_penalty=miss_baja_2_penalty, \n",
    "                    reward_baja_2=reward_baja_2, penalty_baja_2= penalty_baja_2, \n",
    "                    num_epochs=num_epochs, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740a55a-c574-41f0-8193-fa29b40d96c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5569d-6017-4968-af68-2e00da4c3d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5f173-9b1d-49f9-86ec-6ac546f31ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_float('lr', 1e-8, 5e-6, log=True)\n",
    "    print(f\"Using learning_rate: {learning_rate}\")\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 2, 8)\n",
    "    print(f\"Using num_layers: {num_layers}\")\n",
    "    \n",
    "    dropout_rate = trial.suggest_float('dropout', 0.05, 0.4)\n",
    "    print(f\"Using dropout_rate: {dropout_rate}\")\n",
    "\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 6])\n",
    "    factor = trial.suggest_int('factor', 150, 300, step=2)\n",
    "    d_model = num_heads * factor\n",
    "    print(f\"Using num_heads: {num_heads}, d_model: {d_model}\")\n",
    "\n",
    "    dim_factor = trial.suggest_categorical('dim_factor', [1, 2, 3])\n",
    "    dim_feedforward = d_model * dim_factor\n",
    "    print(f\"Using dim_feedforward: {dim_feedforward}\")\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 200, 400, step=8)\n",
    "    print(f\"Using batch_size: {batch_size}\")\n",
    "\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
    "    print(f\"Using weight_decay: {weight_decay}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Initialize model with sampled hyperparameters\n",
    "    model = ActorCriticOrdinalTransformerModel(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout_rate,\n",
    "    num_actor_heads=num_heads,\n",
    "    num_classes=5,\n",
    "    base=10000\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize optimizer with sampled learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize scheduler if needed\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, threshold=0.00001)\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 12  # Or any number of epochs you prefer\n",
    "    pad_value = 0\n",
    "    patience_counter = 0\n",
    "    max_patience = 3\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=200, shuffle=False, pin_memory=True)\n",
    "    sac_loss = RewardWeightedBCEOrdinalLoss().to(device)\n",
    "    \n",
    "\n",
    "    # Initialize variables to track the best metrics\n",
    "    best_ganancia_epoch = float('-inf')\n",
    "    use_thresholds = None\n",
    "    early_stop = False\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training batch loop\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True, ncols=80)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            X_batch, y_batch = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Adjust sequence length if necessary\n",
    "            X_batch, y_batch = adjust_sequence_length(\n",
    "                X_batch, y_batch, max_positive_adjust=6, max_negative_adjust=2, pad_value=pad_value, pad_label=5\n",
    "            )\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze(-1)\n",
    "\n",
    "            mask = create_padding_mask(X_batch, pad_value).to(device)\n",
    "            mask_float = (~mask).float()\n",
    "\n",
    "            # Skip if all sequences are fully padded\n",
    "            all_padded = mask.all(dim=1)\n",
    "            non_empty_indices = (~all_padded).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if non_empty_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            with autocast():\n",
    "                action_probs = model(X_batch, src_key_padding_mask=mask)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if use_thresholds != None:\n",
    "                    predicted_labels = get_class_labels_best(action_probs, thresholds=use_thresholds)\n",
    "                else:   \n",
    "                    predicted_labels = get_class_labels(action_probs, threshold=0.5)\n",
    "                    \n",
    "\n",
    "            # Compute rewards\n",
    "            rewards = compute_rewards_ordinal(preds=predicted_labels,labels=y_batch,mask=mask_float,device=device)\n",
    "\n",
    "            # Compute loss\n",
    "            total_loss = sac_loss(action_probs=action_probs,target_labels=y_batch,rewards=rewards,mask=mask_float,class_weights=None)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": train_loss / len(train_loader)})\n",
    "\n",
    "        # Validation loop\n",
    "        val_loss, best_ganancia, best_argmax_ganancia, best_thresholds, best_ganancia_threshold, _ = validation_loop_sacmh(\n",
    "            model, val_loader, sac_loss, pad_value, device, None,\n",
    "            miss_baja_2_penalty, reward_baja_2, penalty_baja_2\n",
    "        )\n",
    "\n",
    "\n",
    "        # Update best ganancia\n",
    "        if best_ganancia > best_ganancia_epoch:\n",
    "            best_ganancia_epoch = best_ganancia\n",
    "            use_thresholds = None\n",
    "            print(f\"Changing to thresholds: {use_thresholds}\")\n",
    "        else:\n",
    "            scheduler.step(val_loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            patience_counter += 1\n",
    "            print(f\"LR Scheduler Stepping in!: {current_lr}, Patience: {patience_counter}/{max_patience}\")\n",
    "\n",
    "                # Early stopping condition\n",
    "        if patience_counter > max_patience:\n",
    "            print(f\"Early stopping.\")\n",
    "            early_stop = True\n",
    "            break\n",
    "            \n",
    "            \n",
    "        # Optional: Implement pruning\n",
    "        trial.report(best_ganancia, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return best_ganancia_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1b07b-1428-4244-b19f-afbd27dcc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_initial_trials(study):\n",
    "    study.enqueue_trial(Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddc285-c2ab-425a-9c3b-015f281c1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if runOPTUNA:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache() \n",
    "    \n",
    "    study_name = \"ganancia_optimization_study_v8\"\n",
    "    storage = f\"sqlite:///optuna.db\" \n",
    "    grace_period_trials = 3\n",
    "    # Optuna study to maximize ganancia\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=grace_period_trials, n_warmup_steps=4)\n",
    "    study = optuna.create_study(study_name=study_name, direction='maximize', pruner=pruner, storage=storage, load_if_exists=True)  # Maximize ganancia\n",
    "    \n",
    "    #pruner = optuna.pruners.HyperbandPruner(min_resource=3, max_resource=num_epochs, reduction_factor=3)\n",
    "    \n",
    "    #set_initial_trials(study)\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Best trial results\n",
    "    print(f\"Best trial ganancia: {study.best_trial.value}\")\n",
    "    print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f1452-3701-4ae3-9f4d-86b39a0cb5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1e1a9-4177-43d5-840f-52da3eb15b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008ff18-4f08-4331-9395-06a341afdefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a314522-ff6e-4fb9-85bd-ea079eec6951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1caadb-1799-4266-bdcd-5ec7c70f9ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53e27c-a9d6-4df4-88d1-2fc50b4aa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_initial_trials(study):\n",
    "    study.enqueue_trial(Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2dbe4-b8fb-4b8c-93db-0891f26d792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "            #threshold_loss_2 = torch.mean(threshold_baja_2_update_signal)\n",
    "            #model.baja_2_threshold.data += 0.00001 * threshold_loss_2\n",
    "\n",
    "            #threshold_loss_1 = torch.mean(threshold_baja_1_update_signal)\n",
    "            #model.baja_1_threshold.data += 0.00001 * threshold_loss_1\n",
    "\n",
    "            #threshold_loss_c = torch.mean(threshold_continua_update_signal)\n",
    "            #model.continua_threshold.data += 0.00001 * threshold_loss_c\n",
    "\n",
    "            #print(f\"t_loss: {threshold_loss}\")\n",
    "\n",
    "\n",
    "            # Calculate the threshold update signals based on correct/incorrect classifications\n",
    "            #threshold_baja_2_update_signal, threshold_baja_1_update_signal, threshold_continua_update_signal = calculate_threshold_update_signal(action_logits, y_batch, model)\n",
    "            \n",
    "                        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef3381-e812-42e4-a97b-e83566f73c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b94869-362e-461f-8411-d8fb78ff43f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e108-e3e0-4411-ac3c-cf5b3a5254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_optimizer as optim_mod\n",
    "\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "from IPython.display import Javascript, display\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "general_logger = logging.getLogger('general_logger')\n",
    "general_handler = logging.FileHandler('NoForceTransformers_log.txt', mode='w')\n",
    "general_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "general_logger.setLevel(logging.INFO)\n",
    "general_logger.addHandler(general_handler)\n",
    "\n",
    "\n",
    "def log(message):\n",
    "    general_logger.info(message) \n",
    "    print(message)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550eb3e-9b19-401c-94d8-a88a825357e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34605f-ddcb-461d-a080-b0e593f4b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_data_labels = np.load('label_array_filtered_TRAINING3_final.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c713052-0b8d-47c9-b5e8-f8f679314073",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = loaded_data_labels['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b80525-b608-4f49-92ce-d41a49e6b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = label_array[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0dd50a-fa9a-4334-ab99-e6597b15fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee9667-4471-4645-9bb3-01c26f7499cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final corrected conversion function\n",
    "def convert_labels_to_numeric(label_sequence):\n",
    "    numerical_labels = np.full(label_sequence.shape, 5, dtype=int)\n",
    "    numerical_labels[label_sequence == 'OUT'] = 69\n",
    "    baja2_indices = np.where(label_sequence == 'BAJA+2')[0]\n",
    "    for baja2_idx in baja2_indices:\n",
    "        numerical_labels[baja2_idx] = 2\n",
    "        if (baja2_idx + 1) < len(label_sequence) and label_sequence[baja2_idx + 1] == 'BAJA+1':\n",
    "            numerical_labels[baja2_idx + 1] = 1\n",
    "        cont_assignments = [3, 4]\n",
    "        j = baja2_idx - 1\n",
    "        for assign in cont_assignments:\n",
    "            if j >= 0 and label_sequence[j] == 'CONTINUA' and numerical_labels[j] == 5:\n",
    "                numerical_labels[j] = assign\n",
    "                j -= 1\n",
    "            else:\n",
    "                break\n",
    "    baja1_indices = np.where(label_sequence == 'BAJA+1')[0]\n",
    "    assigned_baja1 = baja2_indices + 1\n",
    "    unassigned_baja1 = baja1_indices[~np.isin(baja1_indices, assigned_baja1)]\n",
    "    numerical_labels[unassigned_baja1] = 1\n",
    "    return numerical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec5f61-0abb-4e0b-9e2c-20be317c25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array[1367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be2168-1b08-45a6-907f-4a0a3c75d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_array_regresion = np.array([convert_labels_to_numeric(seq) for seq in label_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dd067-824c-400f-85ff-8114e282025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array_regresion[1367]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbabfdf-2040-4a28-99fd-9d0333d235e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array_regresion[12943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720db224-4266-4c00-b08f-c3f2ab8e65aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array[1368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4a5fd-7168-4b8f-9e6b-2e18033926aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array_regresion[1368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c2691-2386-44bf-84ce-138bed1aef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('label_array_filtered_TRAINING_regresion_final.npz', labels=label_array_regresion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d683004-a371-4ce3-a3d4-f8e72b337f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
